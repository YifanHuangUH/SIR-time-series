{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee3cc16-c854-4087-8238-b1aeda6575b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#equal number in each slice for SIR\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from scipy.linalg import solve_triangular\n",
    "def sir_modified(X, y, num_slices, K):\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    #reduced QR decomposition\n",
    "    Q, R = np.linalg.qr(X)\n",
    "    n_samples, n_features = X.shape\n",
    "    Z = np.sqrt(n_samples) * Q\n",
    "    \n",
    "    V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "    # Step 1: Sort the data by the response variable\n",
    "    sorted_indices = np.argsort(y)\n",
    "    Z_sorted = Z[sorted_indices]\n",
    "    y_sorted = y[sorted_indices]\n",
    "\n",
    "    # Step 2: Divide the data into slices\n",
    "    slice_size = n_samples // num_slices\n",
    "    ph_hat = slice_size/n_samples\n",
    "    slices = []\n",
    "\n",
    "    for i in range(num_slices):\n",
    "        start_idx = i * slice_size\n",
    "        if i < num_slices - 1:\n",
    "            end_idx = (i + 1) * slice_size\n",
    "        else:  # Last slice includes any remaining samples\n",
    "            end_idx = n_samples\n",
    "        slices.append((Z_sorted[start_idx:end_idx], y_sorted[start_idx:end_idx]))\n",
    "    \n",
    "    # Step 3: Compute the means of the predictors within each slice\n",
    "    Z_means = np.array([np.mean(slice_Z, axis=0) for slice_Z, _ in slices])\n",
    "    \n",
    "    # Step 4: Center the predictor means\n",
    "    Z_centered = Z_means - np.mean(Z_means, axis=0)\n",
    "\n",
    "    V_hat = np.add(V_hat,ph_hat * np.matmul(Z_centered.T, Z_centered))\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "    \n",
    "    idx = eigenvalues.argsort()[::-1]  # Get indices that would sort eigenvalues in descending order\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    edr_est = solve_triangular(R, eigenvectors)\n",
    "    # edr_est = solve_triangular(np.sqrt(n_samples) * R, eigenvectors)\n",
    "    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K\n",
    "    edr_est = edr_est[:, K_index]\n",
    "    # edr_est = edr_est.flatten()\n",
    "    if edr_est[0][0] < 0:\n",
    "        edr_est = -edr_est\n",
    "    edr_est = edr_est / np.linalg.norm(edr_est)\n",
    "    \n",
    "    return edr_est.T, V_hat, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c834725-f806-49ec-a162-a5d6c770f6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6663143063279877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sir_modified test on 2*z1t + 3*z2t\n",
    "import sliced\n",
    "from sliced import SlicedInverseRegression\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff = [0.2, 0.8, 0.5, 0.8]\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):\n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    edr, _, R = sir_modified(X, y, H, K=K)\n",
    "    hat += edr\n",
    "\n",
    "hat = hat/n\n",
    "g = hat[0][0]/hat[0][1]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b176904-0add-4535-807d-3e1271916cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6669418312654851"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sir_modified test on 2*z1t + 3*z2t\n",
    "import sliced\n",
    "from sliced import SlicedInverseRegression\n",
    "S = 12\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff1 = 0.2\n",
    "ar_coeff2 = 0.8\n",
    "noise = np.zeros((num_N, n_obs + S))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):\n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs + S)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs + S))\n",
    "    for t in range(0, n_obs + S):\n",
    "        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t-1] + 3*ar_series[1][t-1] + noise[4][t]\n",
    "    y = ar_series[4][1:n_obs+1]\n",
    "    X = np.concatenate([ar_series[i][0:n_obs].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    edr, _, R = sir_modified(X, y, H, K=K)\n",
    "    hat += edr\n",
    "\n",
    "hat = hat/n\n",
    "g = hat[0][0]/hat[0][1]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aefdbd2-be9e-4326-9013-fbad91709cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8341902858951119"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for more complicated SIR for time series examples, use new objective\n",
    "#sir_modified test on 2*z1t + 3*z2t\n",
    "import sliced\n",
    "from sliced import SlicedInverseRegression\n",
    "S = 12\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff1 = 0.2\n",
    "ar_coeff2 = 0.8\n",
    "noise = np.zeros((num_N, n_obs + S))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):\n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs + S)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs + S))\n",
    "    for t in range(0, n_obs + S):\n",
    "        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t-2] + 3*ar_series[1][t-1] + noise[4][t]\n",
    "    y = ar_series[4][2:n_obs+2]\n",
    "    X = np.concatenate([ar_series[i][0:n_obs].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    edr, _, R = sir_modified(X, y, H, K=K)\n",
    "    hat += edr\n",
    "\n",
    "hat = hat/n\n",
    "g = hat[0][0]/hat[0][1]\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daf98a-024d-43c6-8a0f-b0cdb36bbb6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469bc1bc-0c21-4b78-b67b-03be53e89d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035fbbb-0545-440d-8b8d-e05e56117bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793eef80-462d-43a6-a62d-6ef6bd2af9b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34f773d1-4f6e-4c42-afe8-b9101b5687f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.554906 & 0.831899 & 0.000518947 & 0.00016933 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.6670353]]\n"
     ]
    }
   ],
   "source": [
    "#from 0 to Q double sum\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "def test(ar_coeff):\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "    num_N = 5\n",
    "    n_obs = 10000\n",
    "    #######\n",
    "    S = 1\n",
    "    #######\n",
    "    noise = np.zeros((num_N, n_obs+S))\n",
    "    H = 50\n",
    "    P = 4\n",
    "    K = 1\n",
    "    y = [np.zeros((num_N, n_obs+i)) for i in range(S+1)]\n",
    "    hat = [np.zeros((P, 1)) for _ in range(S)]\n",
    "    g = np.zeros((S, 1))\n",
    "    n1 = 0\n",
    "    l = 1  \n",
    "    while n1 < 100:\n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=(n_obs+S))  # Normally distributed noise\n",
    "        ar_series = np.zeros((num_N, n_obs+S))\n",
    "        for t in range(0, n_obs+S):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]\n",
    "            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]\n",
    "            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]\n",
    "            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]\n",
    "        for a in range(0, S+1):\n",
    "            y[a] = ar_series[4][a:n_obs+a]\n",
    "        X = np.concatenate([ar_series[i][0:n_obs].reshape(-1, 1) for i in range(4)], axis=1)\n",
    "        V1 = []\n",
    "        Q2 = []\n",
    "        for a in range(0, S + 1):\n",
    "            _ , M, Q1 = sir_modified(X, y[a], H, K)\n",
    "            V1.append(M)\n",
    "            Q2.append(Q1)\n",
    "        for q in range(1, S + 1):\n",
    "            Q3 = np.zeros((P, P))\n",
    "            phi = ar_coeff\n",
    "            for j in range(P):\n",
    "                for k in range(P):\n",
    "                    #transform back to find V_hat\n",
    "                    Q3[j, k] = sum(sum((phi[j] ** a) * (V1[a])[j, k] * (phi[k] ** a) for a in range(0, l)) for l in range(1, q + 1))\n",
    "                    # Q3[j, k] = sum(sum((phi[j] ** a) * (np.linalg.inv(Q2[a]) @ np.linalg.inv(Q2[a]) @ V1[a] @ (np.linalg.inv(Q2[a])).T @ Q2[a])[j, k] * (phi[k] ** a) for a in range(0, l)) for l in range(1, q + 1))\n",
    "            #Q2[a] @ V[a] @ Q2[a].T by sample covariance matrix\n",
    "            eigenvalues1, eigenvectors1 = np.linalg.eig(Q3)\n",
    "            edr_est = solve_triangular(Q2[0], eigenvectors1)\n",
    "            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K\n",
    "            K_largest_eigenvectors = edr_est[:, K_index]\n",
    "            # K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "            edr_est = K_largest_eigenvectors\n",
    "            if edr_est[0] < 0:\n",
    "                edr_est = -edr_est\n",
    "            edr_est = edr_est / np.linalg.norm(edr_est)\n",
    "            hat[q - 1] += edr_est\n",
    "        n1 += 1\n",
    "    \n",
    "    for i in range(S):\n",
    "        hat[i] = hat[i] / n\n",
    "        g[i] = hat[i][0] / hat[i][1]\n",
    "    array = np.array(hat)\n",
    "    print(tabulate(array, tablefmt='latex'))\n",
    "    print(g)\n",
    "\n",
    "# Example usage\n",
    "ar_coeff = [0.2, 0.5, 0.2, 0.2]\n",
    "test(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dae2d-d51e-453f-a32f-bc73ec573881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d2af7bc-3b93-4724-a77f-5a607e7b14c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.555345 & 0.831611 & 0.000195165 & -0.000383046 \\\\\n",
      " 0.547716 & 0.836655 & 0.000228    & -0.000346709 \\\\\n",
      " 0.54483  & 0.838537 & 0.000242285 & -0.000327503 \\\\\n",
      " 0.543372 & 0.839482 & 0.00024958  & -0.000315886 \\\\\n",
      " 0.542497 & 0.840048 & 0.000253931 & -0.00030842  \\\\\n",
      " 0.541915 & 0.840424 & 0.000256824 & -0.000303425 \\\\\n",
      " 0.541499 & 0.840691 & 0.000258887 & -0.000299868 \\\\\n",
      " 0.541188 & 0.840892 & 0.000260431 & -0.000297201 \\\\\n",
      " 0.540946 & 0.841047 & 0.000261632 & -0.00029513  \\\\\n",
      " 0.540752 & 0.841172 & 0.000262592 & -0.000293476 \\\\\n",
      " 0.540594 & 0.841273 & 0.000263376 & -0.000292123 \\\\\n",
      " 0.540462 & 0.841358 & 0.00026403  & -0.000290997 \\\\\n",
      " 0.540351 & 0.84143  & 0.000264583 & -0.000290044 \\\\\n",
      " 0.540255 & 0.841491 & 0.000265057 & -0.000289228 \\\\\n",
      " 0.540172 & 0.841544 & 0.000265467 & -0.000288521 \\\\\n",
      " 0.5401   & 0.841591 & 0.000265827 & -0.000287902 \\\\\n",
      " 0.540036 & 0.841632 & 0.000266143 & -0.000287357 \\\\\n",
      " 0.539979 & 0.841668 & 0.000266425 & -0.000286872 \\\\\n",
      " 0.539928 & 0.841701 & 0.000266677 & -0.000286438 \\\\\n",
      " 0.539883 & 0.84173  & 0.000266903 & -0.000286048 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.66779402]\n",
      " [0.6546492 ]\n",
      " [0.64973941]\n",
      " [0.64727034]\n",
      " [0.64579338]\n",
      " [0.6448116 ]\n",
      " [0.64411191]\n",
      " [0.64358804]\n",
      " [0.64318113]\n",
      " [0.64285594]\n",
      " [0.64259011]\n",
      " [0.64236873]\n",
      " [0.64218153]\n",
      " [0.64202115]\n",
      " [0.64188221]\n",
      " [0.64176069]\n",
      " [0.64165349]\n",
      " [0.64155824]\n",
      " [0.64147304]\n",
      " [0.64139637]]\n"
     ]
    }
   ],
   "source": [
    "#from 0 to Q double sum\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "def test1(ar_coeff):\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "    num_N = 5\n",
    "    n_obs = 10000\n",
    "    S = 20\n",
    "    noise = np.zeros((num_N, n_obs+S))\n",
    "    H = 50\n",
    "    P = 4\n",
    "    K = 1\n",
    "    y = [np.zeros((num_N, n_obs+i)) for i in range(S+1)]\n",
    "    hat = [np.zeros((P, 1)) for _ in range(S)]\n",
    "    g = np.zeros((S, 1))\n",
    "    n1 = 0\n",
    "    l = 1  \n",
    "    while n1 < 100:\n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=(n_obs+S))  # Normally distributed noise\n",
    "        ar_series = np.zeros((num_N, n_obs+S))\n",
    "        for t in range(0, n_obs+S):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]\n",
    "            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]\n",
    "            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]\n",
    "            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]\n",
    "        for a in range(0, S+1):\n",
    "            y[a] = ar_series[4][a:n_obs+a]\n",
    "        X = np.concatenate([ar_series[i][0:n_obs].reshape(-1, 1) for i in range(4)], axis=1)\n",
    "        V1 = []\n",
    "        Q2 = []\n",
    "        for a in range(0, S + 1):\n",
    "            _ , M, Q1 = sir_modified(X, y[a], H, K)\n",
    "            V1.append(M)\n",
    "            Q2.append(Q1)\n",
    "        for q in range(1, S + 1):\n",
    "            Q3 = np.zeros((P, P))\n",
    "            phi = ar_coeff\n",
    "            for j in range(P):\n",
    "                for k in range(P):\n",
    "                    #transform back to find V_hat\n",
    "                    Q3[j, k] = sum(sum((phi[j] ** a) * (np.linalg.inv(Q2[a]) @ V1[a] @ (np.linalg.inv(Q2[a])).T)[j, k] * (phi[k] ** a) for a in range(0, l)) for l in range(1, q + 1))\n",
    "                    # Q3[j, k] = sum(sum((phi[j] ** a) * (np.linalg.inv(Q2[a]) @ np.linalg.inv(Q2[a]) @ V1[a] @ (np.linalg.inv(Q2[a])).T @ Q2[a])[j, k] * (phi[k] ** a) for a in range(0, l)) for l in range(1, q + 1))\n",
    "            #Q2[a] @ V[a] @ Q2[a].T by sample covariance matrix\n",
    "            eigenvalues1, eigenvectors1 = np.linalg.eig(Q3)\n",
    "            # edr_est = solve_triangular(Q2[0], eigenvectors1)\n",
    "            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K\n",
    "            # K_largest_eigenvectors = edr_est[:, K_index]\n",
    "            K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "            edr_est = K_largest_eigenvectors\n",
    "            if edr_est[0] < 0:\n",
    "                edr_est = -edr_est\n",
    "            edr_est = edr_est / np.linalg.norm(edr_est)\n",
    "            hat[q - 1] += edr_est\n",
    "        n1 += 1\n",
    "    \n",
    "    for i in range(S):\n",
    "        hat[i] = hat[i] / n\n",
    "        g[i] = hat[i][0] / hat[i][1]\n",
    "    array = np.array(hat)\n",
    "    print(tabulate(array, tablefmt='latex'))\n",
    "    print(g)\n",
    "\n",
    "# Example usage\n",
    "ar_coeff = [0.2, 0.5, 0.5, 0.8]\n",
    "test1(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea4e0905-08fa-431e-a9f2-3abeff653735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.555065 & 0.831795 & 8.99199e-05 & -7.9383e-05  \\\\\n",
      " 0.554314 & 0.832296 & 0.000117502 & -9.20674e-05 \\\\\n",
      " 0.554055 & 0.832468 & 0.000126326 & -9.68162e-05 \\\\\n",
      " 0.553926 & 0.832554 & 0.000130774 & -9.93127e-05 \\\\\n",
      " 0.553848 & 0.832606 & 0.000133441 & -0.000100866 \\\\\n",
      " 0.553796 & 0.83264  & 0.000135219 & -0.0001019   \\\\\n",
      " 0.553759 & 0.832665 & 0.000136488 & -0.000102639 \\\\\n",
      " 0.553731 & 0.832683 & 0.00013744  & -0.000103194 \\\\\n",
      " 0.55371  & 0.832698 & 0.000138181 & -0.000103626 \\\\\n",
      " 0.553693 & 0.832709 & 0.000138773 & -0.000103972 \\\\\n",
      " 0.553678 & 0.832719 & 0.000139258 & -0.000104256 \\\\\n",
      " 0.553667 & 0.832726 & 0.000139661 & -0.000104492 \\\\\n",
      " 0.553657 & 0.832733 & 0.000140003 & -0.000104692 \\\\\n",
      " 0.553648 & 0.832739 & 0.000140296 & -0.000104863 \\\\\n",
      " 0.553641 & 0.832744 & 0.00014055  & -0.000105011 \\\\\n",
      " 0.553634 & 0.832748 & 0.000140772 & -0.000105141 \\\\\n",
      " 0.553629 & 0.832752 & 0.000140968 & -0.000105256 \\\\\n",
      " 0.553624 & 0.832755 & 0.000141142 & -0.000105358 \\\\\n",
      " 0.553619 & 0.832758 & 0.000141298 & -0.000105449 \\\\\n",
      " 0.553615 & 0.832761 & 0.000141438 & -0.000105531 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.66730933]\n",
      " [0.66600588]\n",
      " [0.66555735]\n",
      " [0.66533281]\n",
      " [0.66519816]\n",
      " [0.66510844]\n",
      " [0.66504437]\n",
      " [0.66499633]\n",
      " [0.66495898]\n",
      " [0.6649291 ]\n",
      " [0.66490466]\n",
      " [0.66488429]\n",
      " [0.66486706]\n",
      " [0.66485229]\n",
      " [0.66483949]\n",
      " [0.66482829]\n",
      " [0.66481841]\n",
      " [0.66480963]\n",
      " [0.66480177]\n",
      " [0.6647947 ]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2, 0.3, 0.5, 0.8]\n",
    "test1(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4388d-82df-4d4c-8626-539b3fe457a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c98f8-b925-4ed8-a098-36c4ea6f56e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3a868-1939-4251-accc-1661aa0090e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2033082-c048-44fc-9bc1-409a4f37acea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b5d705-4557-4921-b0e9-92f70677a956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
