{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106731a6-d34a-4d8f-89fc-280d6065e35e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d652bf8-1044-494a-8193-c00ed38f2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.linalg import sqrtm\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt(np.sum((point1 - point2)**2)) \n",
    "# equal slice\n",
    "def sir_1(X, y, H, K):\n",
    "    Z = X-np.mean(X, axis=0)\n",
    "    width = (np.max(y) - np.min(y)) / H\n",
    "    V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "    for h in range(H):\n",
    "        h_index = np.logical_and(np.min(y)+h*width <= y, y < np.min(y)+(h+1)*width)\n",
    "        ph_hat = np.mean(h_index)\n",
    "        if ph_hat == 0:\n",
    "            continue\n",
    "        mh = np.mean(Z[h_index, :], axis=0)\n",
    "        V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K\n",
    "    K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "    edr_est =  K_largest_eigenvectors       \n",
    "    return  edr_est\n",
    "    \n",
    "# equal slice\n",
    "def sir_1_time_series(X, y, s, H, K):\n",
    "    from collections import deque\n",
    "    def move_left(queue, steps):\n",
    "        # Length of the queue\n",
    "        n = len(queue)\n",
    "        # Create a new list filled with 0\n",
    "        new_queue = [0] * n\n",
    "        \n",
    "        # Move elements to the left by the specified number of steps\n",
    "        for i in range(n):\n",
    "            new_position = i - steps\n",
    "            if new_position >= 0:\n",
    "                new_queue[new_position] = queue[i]\n",
    "        \n",
    "        return new_queue\n",
    "       \n",
    "    Z = X-np.mean(X, axis=0)\n",
    "    width = (np.max(y) - np.min(y)) / H\n",
    "    V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "    for h in range(H):\n",
    "        h_index = np.logical_and(np.min(y)+h*width <= y, y < np.min(y)+(h+1)*width)\n",
    "        # queue moves s locations backward\n",
    "        h_index = move_left(h_index, s) \n",
    "        # h_index = h_index[len(y) - X.shape[0]:]\n",
    "        # h_index = h_index[:X.shape[0]]\n",
    "        ph_hat = np.mean(h_index)\n",
    "        if ph_hat == 0:\n",
    "            continue\n",
    "        # if np.all((h_index == 1) < X.shape[0]):\n",
    "        mh = np.mean(Z[h_index, :], axis=0)\n",
    "        V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))\n",
    "        # else:\n",
    "        #     h_index = h_index\n",
    "        #     mh = np.mean(Z[h_index, :], axis=0)\n",
    "        #     V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K\n",
    "    K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "    edr_est =  K_largest_eigenvectors         \n",
    "    return  (V_hat, edr_est, eigenvalues ** 2)\n",
    "    \n",
    "#equal number in each slice\n",
    "#queue structure\n",
    "def sir_1_time_series1(X, y, s, num_slices, K):\n",
    "    n_samples, n_features = X.shape\n",
    "    V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "    # Step 1: Sort the data by the response variable\n",
    "    sorted_indices = np.argsort(y)\n",
    "    # sorted_indices1 = np.argsort(y[0:X.shape[0]])\n",
    "    # if len(sorted_indices) > X.shape[0]:\n",
    "    #     X_sorted = X[0:X.shape[0]][sorted_indices1]\n",
    "    # else:\n",
    "    X_sorted = X[sorted_indices]\n",
    "    y_sorted = y[sorted_indices]\n",
    "    # Step 2: Divide the data into slices\n",
    "    slice_size = n_samples // num_slices\n",
    "    ph_hat = slice_size/n_samples\n",
    "    slices = []\n",
    "    \n",
    "    for i in range(num_slices):\n",
    "        start_idx = i * slice_size\n",
    "        if i < num_slices - 1:\n",
    "            end_idx = (i + 1) * slice_size\n",
    "        else:  # Last slice includes any remaining samples\n",
    "            end_idx = n_samples\n",
    "        if start_idx-s>0:\n",
    "            slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))\n",
    "        else:\n",
    "            slices.append((X[sorted_indices[0:end_idx-s]], y_sorted[start_idx:end_idx]))\n",
    "    \n",
    "    # Step 3: Compute the means of the predictors within each slice\n",
    "    X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])\n",
    "    \n",
    "    # Step 4: Center the predictor means\n",
    "    X_centered = X_means - np.mean(X_means, axis=0)\n",
    "    # Check for NaN or inf values in V_hat\n",
    "    if np.isnan(V_hat).any() or np.isinf(V_hat).any():\n",
    "        # Handle NaN or inf values appropriately\n",
    "        # For example, replace NaN values with 0 and inf values with a large number\n",
    "        V_hat[np.isnan(V_hat)] = 0\n",
    "        V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat\n",
    "        \n",
    "    V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K\n",
    "    K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "    edr_est =  K_largest_eigenvectors\n",
    "    \n",
    "    return (V_hat, edr_est, eigenvalues ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3defc2-ada7-4437-b5cb-95105232076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#equal number in each slice for SIR\n",
    "def sir_11(X, y, num_slices, K):\n",
    "    n_samples, n_features = X.shape\n",
    "    V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "    # Step 1: Sort the data by the response variable\n",
    "    sorted_indices = np.argsort(y)\n",
    "    X_sorted = X[sorted_indices]\n",
    "    y_sorted = y[sorted_indices]\n",
    "    \n",
    "    # Step 2: Divide the data into slices\n",
    "    slice_size = n_samples // num_slices\n",
    "    ph_hat = slice_size/n_samples\n",
    "    slices = []\n",
    "    \n",
    "    for i in range(num_slices):\n",
    "        start_idx = i * slice_size\n",
    "        if i < num_slices - 1:\n",
    "            end_idx = (i + 1) * slice_size\n",
    "        else:  # Last slice includes any remaining samples\n",
    "            end_idx = n_samples\n",
    "        slices.append((X_sorted[start_idx:end_idx], y_sorted[start_idx:end_idx]))\n",
    "    \n",
    "    # Step 3: Compute the means of the predictors within each slice\n",
    "    X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])\n",
    "    \n",
    "    # Step 4: Center the predictor means\n",
    "    X_centered = X_means - np.mean(X_means, axis=0)\n",
    "    \n",
    "    V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K\n",
    "    K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "    edr_est =  K_largest_eigenvectors\n",
    "    \n",
    "    return edr_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8222b45-5723-4b4e-90e2-b84fa569d11f",
   "metadata": {},
   "source": [
    "### Above are functions for SIR, SIR different output(V_hat, edr_est, eigenvalues ** 2 and edr_est), SIR for equal number in each slice and SIR with equal slice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc3ebb-3bce-4040-94bd-8b030103584a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671069b6-1c27-4bb5-8f9c-2575323e6896",
   "metadata": {},
   "source": [
    "For model\n",
    "\\begin{align}\n",
    "   & y_t = 2z_{1,t-1} + 3z_{2,t-1} +\\epsilon_t\n",
    "    \\label{model:time series 2}\n",
    "\\end{align}\n",
    "with four components $z_1 \\sim \\text{AR}(1)$ with $\\phi_1$, $z_2 \\sim \\text{AR}(1)$ with $\\phi_2$, $z_3 \\sim \\text{ARMA}(1, 1)$ with $\\phi = 0.3$ and $\\theta = 0.4$ and $z_4 \\sim \\text{MA}(1)$ with $\\theta = -0.4$. Dimension $p = 4$. $K = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e12d9-c131-4043-b458-bf65c2abeb19",
   "metadata": {},
   "source": [
    "## Results under Sir_1(equal slice) and Sir_11(equal number in each slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c61b90-4232-4a3d-be6b-b836c092734b",
   "metadata": {},
   "source": [
    "### coefficient 0.2 0.2, correct direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "565d947b-1906-4ac8-8647-3ce74a187d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 0.55022728]\n",
      " [ 0.82705784]\n",
      " [-0.00266489]\n",
      " [-0.00193875]]\n",
      "Equal slice ratio:[0.66528271]\n",
      "Equal number in each slice direction:[[ 5.47185641e-01]\n",
      " [ 8.29002510e-01]\n",
      " [-3.05165667e-03]\n",
      " [-7.34620269e-04]]\n",
      "Equal number in each slice ratio:[0.66005306]\n"
     ]
    }
   ],
   "source": [
    "#0.2 0.2 correct direction\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff1 = 0.2 \n",
    "ar_coeff2 = 0.2\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65e84c-b191-4bad-aa24-01d826f5d3c9",
   "metadata": {},
   "source": [
    "### coefficient 0.2 0.8, incorrect direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "028c0ac8-d4ce-41aa-acef-99402f84dd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 6.04709463e-02]\n",
      " [ 9.97990092e-01]\n",
      " [-6.48172944e-04]\n",
      " [-3.21339522e-04]]\n",
      "Equal slice ratio:[0.06059273]\n",
      "Equal number in each slice direction:[[ 6.05792852e-02]\n",
      " [ 9.97980320e-01]\n",
      " [-5.72619709e-04]\n",
      " [-2.59907093e-04]]\n",
      "Equal number in each slice ratio:[0.06070188]\n"
     ]
    }
   ],
   "source": [
    "#0.2 0.8 incorrect direction\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff1 = 0.2 \n",
    "ar_coeff2 = 0.8\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d18057-89f6-46d3-8b0d-4fa5c7f86a75",
   "metadata": {},
   "source": [
    "### coefficient 0.8 0.8, correct direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "781619fa-0024-447a-8750-9d1e4a96b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 5.51164249e-01]\n",
      " [ 8.33834171e-01]\n",
      " [ 7.05916426e-04]\n",
      " [-2.35679570e-04]]\n",
      "Equal slice ratio:[0.66099983]\n",
      "Equal number in each slice direction:[[ 5.51118853e-01]\n",
      " [ 8.33869763e-01]\n",
      " [ 7.60820593e-04]\n",
      " [-2.48872414e-04]]\n",
      "Equal number in each slice ratio:[0.66091718]\n"
     ]
    }
   ],
   "source": [
    "#0.8 0.8 correct direction\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff1 = 0.8 \n",
    "ar_coeff2 = 0.8\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "Hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445af9f4-8ba0-4402-9d77-9ed7e9b7af18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "942563b1-936e-4e15-a629-aca601f35022",
   "metadata": {},
   "source": [
    "## SIR for model\n",
    "\\begin{align}\n",
    "   & y_t = 2z_{1,t} + 3z_{2,t} +\\epsilon_t\n",
    "\\end{align}\n",
    "with four components $z_i \\sim \\text{AR}(1)$ with $\\phi_i, i = 1, \\ldots, 4$. I denote the model as NO(0.2, 0.2, 0.2, 0.2) if $\\phi_i = 0.2, i = 1, \\ldots, 4$. $K = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb3733-ccb5-4644-9e09-18dabd0bd00e",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2, 0.2, 0.2, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88b249b7-497f-42b6-9d5c-7e29c2134296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 5.55096135e-01]\n",
      " [ 8.31595927e-01]\n",
      " [ 9.87298312e-04]\n",
      " [-1.54833445e-04]]\n",
      "Equal slice ratio:[0.66750704]\n",
      "Equal number in each slice direction:[[1.10950932e+00]\n",
      " [1.66323899e+00]\n",
      " [1.83722916e-03]\n",
      " [3.52549362e-04]]\n",
      "Equal number in each slice ratio:[0.66707751]\n"
     ]
    }
   ],
   "source": [
    "#ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H=H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1fd9d-7cc7-49ff-8ad0-0ed0a917140b",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2, 0.2, 0.8, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0575e46a-dd35-4670-a94b-f42f371b805c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 5.54614799e-01]\n",
      " [ 8.31551840e-01]\n",
      " [ 2.93844972e-03]\n",
      " [-3.30158714e-04]]\n",
      "Equal slice ratio:[0.66696359]\n",
      "Equal number in each slice direction:[[ 1.66417676e+00]\n",
      " [ 2.49475284e+00]\n",
      " [ 4.59979919e-03]\n",
      " [-1.02698341e-04]]\n",
      "Equal number in each slice ratio:[0.6670708]\n"
     ]
    }
   ],
   "source": [
    "#ar_coeff = [0.2, 0.2, 0.8, 0.8]\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff = [0.2, 0.2, 0.8, 0.8]\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H=H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d9d71-bf2d-4b87-b00b-b3f136e28657",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2, 0.5, 0.8, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e943dc2-46c5-4611-8590-00a9fdccd9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[0.46189184]\n",
      " [0.88630596]\n",
      " [0.0042448 ]\n",
      " [0.00288614]]\n",
      "Equal slice ratio:[0.52114266]\n",
      "Equal number in each slice direction:[[1.01290085]\n",
      " [1.7202301 ]\n",
      " [0.00497268]\n",
      " [0.00291084]]\n",
      "Equal number in each slice ratio:[0.58881707]\n"
     ]
    }
   ],
   "source": [
    "#ar_coeff = [0.2, 0.5, 0.8, 0.8]\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff = [0.2, 0.5, 0.8, 0.8]\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H=H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17c969-d1f8-44bd-9133-0074acd0821d",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2, 0.5, 0.5, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39205120-cd13-4d56-8f43-b00659c148d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal slice direction:[[ 4.63183556e-01]\n",
      " [ 8.85775180e-01]\n",
      " [ 1.06089700e-04]\n",
      " [-9.76717593e-04]]\n",
      "Equal slice ratio:[0.52291323]\n",
      "Equal number in each slice direction:[[1.47604378e+00]\n",
      " [2.60602530e+00]\n",
      " [5.14360204e-03]\n",
      " [1.92624909e-03]]\n",
      "Equal number in each slice ratio:[0.56639656]\n"
     ]
    }
   ],
   "source": [
    "#ar_coeff = [0.2, 0.5, 0.5, 0.8]\n",
    "num_N = 5\n",
    "n_obs = 10000\n",
    "ar_coeff = [0.2, 0.5, 0.5, 0.8]\n",
    "noise = np.zeros((num_N, n_obs))\n",
    "n = 100\n",
    "H = 50\n",
    "K = 1\n",
    "hat = 0\n",
    "for a in range(n):    \n",
    "    for h in range(num_N):\n",
    "        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "    ar_series = np.zeros((num_N, n_obs))\n",
    "    for t in range(0, n_obs):\n",
    "        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "    y = ar_series[4]\n",
    "    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "    sir_1_result = sir_1(X, y, H=H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_1_result[0]<0:\n",
    "        sir_1_result = -sir_1_result\n",
    "    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)\n",
    "    hat = hat + sir_1_result \n",
    "    sir_11_result = sir_11(X, y, H, K=K)\n",
    "    #after adding normalization and modifying the first coefficient\n",
    "    if sir_11_result[0]<0:\n",
    "        sir_11_result = -sir_11_result\n",
    "    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)\n",
    "    Hat = Hat + sir_11_result\n",
    "print(f\"Equal slice direction:{hat/n}\")\n",
    "hat1 = hat/n\n",
    "print(f\"Equal slice ratio:{hat1[0]/hat1[1]}\")\n",
    "print(f\"Equal number in each slice direction:{Hat/n}\")\n",
    "Hat1 = Hat/n\n",
    "print(f\"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bc6db-15bd-4e53-814e-48e4f1a65722",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SIR for Time series new objective(Double sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce111cb1-4683-49ec-9252-6f492e557ea6",
   "metadata": {},
   "source": [
    "Code below is for this derivation. When taking the sum across all $q$, it is easily seen that \n",
    "$$\n",
    "\\sum_q diag\\left[ (\\mathbf{W} \\circ \\Phi^{\\circ q})^T Cov(\\mathbb{E}(\\mathbf{X}_{t-q}|y) (\\mathbf{W} \\circ\\Phi^{\\circ q})\\right]=diag(\\mathbf{W}^T\\sum_q \\mathbf{V}^{[1:q]}\\mathbf{W}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{V}^{[1:Q]}_{jk}=\\sum_{q=1}^Q\\mathbf{V}^q_{jk}\\cdot \\phi_j^q\\cdot \\phi_k^q.\n",
    "$$\n",
    "I'm wondering, is it $\\text{diag}(W^T V^{[1: q]} W)$, not $\\text{diag}(\\mathbf{W}^T\\sum_q \\mathbf{V}^{[1:q]}\\mathbf{W}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898f8a47-9100-4bed-af1a-7c650cc53261",
   "metadata": {},
   "source": [
    "## Use equal slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8836a587-541f-424a-abbb-87c79d6c9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 0 to Q double sum\n",
    "from tabulate import tabulate\n",
    "def NEWSIR(ar_coeff, T):\n",
    "    num_N = 5\n",
    "    n_obs = 1000\n",
    "    noise = np.zeros((num_N, n_obs))\n",
    "    n = 100\n",
    "    H = 50\n",
    "    P = 4\n",
    "    K = 1\n",
    "    S = 20\n",
    "    hat = [np.zeros((P, 1)) for i in range(S)]\n",
    "    # ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "    g = np.zeros((S, 1))\n",
    "    for w in range(n):    \n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "        ar_series = np.zeros((num_N, n_obs))\n",
    "        for t in range(0, n_obs):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "            ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "        y = ar_series[4]\n",
    "        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "        V = []\n",
    "        for a in range(0,S+1):\n",
    "            M, edr, lam1 = T(X, y, a, H, K)\n",
    "            V.append(M)\n",
    "        for q in range(1, S+1):\n",
    "            Q = np.zeros((P, P))\n",
    "            phi = ar_coeff\n",
    "            for j in range(P):\n",
    "                for k in range(P):\n",
    "                    Q[j,k] = sum(sum(phi[j]**a * V[a][j,k] * phi[k]**a for a in range(0,l)) for l in range(1, q+1)) #double sum\n",
    "            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)\n",
    "            K_index = np.argpartition(np.abs(eigenvalues1), P-K) >= P-K\n",
    "            K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "            edr_est =  K_largest_eigenvectors\n",
    "            if edr_est[0]<0:\n",
    "                edr_est = -edr_est\n",
    "            edr_est = edr_est/np.linalg.norm(edr_est)\n",
    "            hat[q-1] += edr_est       \n",
    "    for i in range(S):\n",
    "        hat[i] = hat[i]/n\n",
    "        g[i] = hat[i][0] / hat[i][1]\n",
    "    array = np.array(hat)\n",
    "    print(tabulate(array, tablefmt='latex'))\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5d8fa4-5009-4664-882b-ee9f1518ada9",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.2,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a433cdab-9e0b-4379-80ae-10a77f501d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.554219 & 0.830436 & 0.0011332  & 0.00473857 \\\\\n",
      " 0.549475 & 0.832778 & 0.00308414 & 0.00395092 \\\\\n",
      " 0.547393 & 0.83338  & 0.00374569 & 0.00358167 \\\\\n",
      " 0.546266 & 0.833628 & 0.00407116 & 0.00336832 \\\\\n",
      " 0.545562 & 0.833758 & 0.00426425 & 0.00322942 \\\\\n",
      " 0.545082 & 0.833836 & 0.00439198 & 0.00313179 \\\\\n",
      " 0.544733 & 0.833888 & 0.00448267 & 0.00305942 \\\\\n",
      " 0.544468 & 0.833924 & 0.00455039 & 0.00300364 \\\\\n",
      " 0.544261 & 0.833951 & 0.00460286 & 0.00295934 \\\\\n",
      " 0.544093 & 0.833972 & 0.0046447  & 0.00292329 \\\\\n",
      " 0.543956 & 0.833988 & 0.00467886 & 0.0028934  \\\\\n",
      " 0.54384  & 0.834002 & 0.00470725 & 0.0028682  \\\\\n",
      " 0.543742 & 0.834012 & 0.00473124 & 0.00284668 \\\\\n",
      " 0.543658 & 0.834022 & 0.00475176 & 0.00282809 \\\\\n",
      " 0.543585 & 0.834029 & 0.00476952 & 0.00281186 \\\\\n",
      " 0.54352  & 0.834036 & 0.00478504 & 0.00279757 \\\\\n",
      " 0.543464 & 0.834042 & 0.00479872 & 0.00278489 \\\\\n",
      " 0.543413 & 0.834047 & 0.00481087 & 0.00277357 \\\\\n",
      " 0.543368 & 0.834051 & 0.00482173 & 0.0027634  \\\\\n",
      " 0.543327 & 0.834055 & 0.00483149 & 0.00275421 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.66738332]\n",
      " [0.65980977]\n",
      " [0.6568344 ]\n",
      " [0.65528738]\n",
      " [0.65434181]\n",
      " [0.65370426]\n",
      " [0.65324535]\n",
      " [0.65289925]\n",
      " [0.65262893]\n",
      " [0.65241195]\n",
      " [0.65223396]\n",
      " [0.65208531]\n",
      " [0.65195929]\n",
      " [0.65185111]\n",
      " [0.65175723]\n",
      " [0.65167499]\n",
      " [0.65160235]\n",
      " [0.65153773]\n",
      " [0.65147986]\n",
      " [0.65142774]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.2,0.2]\n",
    "NEWSIR(ar_coeff, sir_1_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92aba1-8f7c-4d01-bdd4-7bc68f317b6d",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0e1edfd1-fa21-4136-83fc-170249526cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.547153 & 0.830722 & -0.0114239  & -0.0049901   \\\\\n",
      " 0.419096 & 0.632559 & -0.0360375  & -0.000328941 \\\\\n",
      " 0.331373 & 0.480581 & -0.0566771  &  0.00678603  \\\\\n",
      " 0.300021 & 0.421163 & -0.0214941  & -0.0191334   \\\\\n",
      " 0.282287 & 0.395401 & -0.00811907 & -0.0246749   \\\\\n",
      " 0.269339 & 0.375329 & -0.0271879  & -0.0412054   \\\\\n",
      " 0.259358 & 0.361838 & -0.0289973  & -0.0423478   \\\\\n",
      " 0.251284 & 0.351705 & -0.0284046  & -0.0415472   \\\\\n",
      " 0.24255  & 0.341601 & -0.0254017  & -0.038518    \\\\\n",
      " 0.238017 & 0.326509 & -0.0385891  & -0.0498715   \\\\\n",
      " 0.235093 & 0.322698 & -0.0386271  & -0.0498593   \\\\\n",
      " 0.232531 & 0.319394 & -0.0385321  & -0.0497285   \\\\\n",
      " 0.230352 & 0.316598 & -0.0384197  & -0.0495571   \\\\\n",
      " 0.228492 & 0.31422  & -0.0383301  & -0.0493624   \\\\\n",
      " 0.22689  & 0.312175 & -0.0382808  & -0.0491494   \\\\\n",
      " 0.225492 & 0.310391 & -0.038279   & -0.0489195   \\\\\n",
      " 0.224258 & 0.308815 & -0.0383264  & -0.0486731   \\\\\n",
      " 0.223155 & 0.307403 & -0.0384214  & -0.0484104   \\\\\n",
      " 0.222158 & 0.306122 & -0.0385605  & -0.0481318   \\\\\n",
      " 0.221247 & 0.304945 & -0.0387385  & -0.0478379   \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.65864743]\n",
      " [0.66254083]\n",
      " [0.68952494]\n",
      " [0.71236259]\n",
      " [0.71392628]\n",
      " [0.71760983]\n",
      " [0.71677962]\n",
      " [0.71447517]\n",
      " [0.71003909]\n",
      " [0.72897732]\n",
      " [0.72852252]\n",
      " [0.72803937]\n",
      " [0.72758601]\n",
      " [0.72717362]\n",
      " [0.72680401]\n",
      " [0.72647599]\n",
      " [0.72618729]\n",
      " [0.72593535]\n",
      " [0.72571759]\n",
      " [0.72553146]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.8,0.8]\n",
    "NEWSIR(ar_coeff,sir_1_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778b5e0-1b39-4c6a-ba14-e2afac18c1bd",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d6704685-bb34-43d3-b866-5b41168ed49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.459295 & 0.880704 & -0.00714909 & 0.00991775 \\\\\n",
      " 0.339796 & 0.730175 &  0.021341   & 0.0268801  \\\\\n",
      " 0.264865 & 0.578714 &  0.0433002  & 0.0586561  \\\\\n",
      " 0.225784 & 0.488499 &  0.0400155  & 0.0408523  \\\\\n",
      " 0.205847 & 0.449643 &  0.0562121  & 0.0513033  \\\\\n",
      " 0.192463 & 0.425626 &  0.0527989  & 0.0319704  \\\\\n",
      " 0.183164 & 0.406909 &  0.0702575  & 0.0168734  \\\\\n",
      " 0.17638  & 0.397186 &  0.0735386  & 0.0160639  \\\\\n",
      " 0.171211 & 0.389644 &  0.0761837  & 0.0154874  \\\\\n",
      " 0.167149 & 0.383234 &  0.0607322  & 0.024511   \\\\\n",
      " 0.163877 & 0.378327 &  0.0625266  & 0.0241779  \\\\\n",
      " 0.161201 & 0.369238 &  0.0566731  & 0.0418287  \\\\\n",
      " 0.159029 & 0.3576   &  0.0736535  & 0.0507315  \\\\\n",
      " 0.157197 & 0.354882 &  0.0747357  & 0.0506117  \\\\\n",
      " 0.155629 & 0.35254  &  0.0756595  & 0.0505135  \\\\\n",
      " 0.154299 & 0.34054  &  0.0867034  & 0.0364445  \\\\\n",
      " 0.153145 & 0.335085 &  0.0734339  & 0.0225382  \\\\\n",
      " 0.152155 & 0.319755 &  0.0878027  & 0.0268339  \\\\\n",
      " 0.151319 & 0.318527 &  0.0884297  & 0.0267761  \\\\\n",
      " 0.150574 & 0.31743  &  0.0889842  & 0.026726   \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.52150814]\n",
      " [0.46536227]\n",
      " [0.45767859]\n",
      " [0.46220009]\n",
      " [0.45780044]\n",
      " [0.45218791]\n",
      " [0.45013502]\n",
      " [0.44407371]\n",
      " [0.43940207]\n",
      " [0.4361533 ]\n",
      " [0.43316259]\n",
      " [0.4365788 ]\n",
      " [0.44471364]\n",
      " [0.44295587]\n",
      " [0.44145215]\n",
      " [0.45310276]\n",
      " [0.45703424]\n",
      " [0.47584948]\n",
      " [0.47505811]\n",
      " [0.47435209]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.8,0.8]\n",
    "NEWSIR(ar_coeff,sir_1_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a7c02-0a9d-4414-8fb1-dfa7f6f28614",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.5,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4a487bf9-77d2-48d6-ac5b-1881af1296cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.453712 & 0.886373 & 0.0033857 & -0.00442161 \\\\\n",
      " 0.37972  & 0.80695  & 0.033548  & -0.0210339  \\\\\n",
      " 0.321667 & 0.703135 & 0.0314533 & -0.0626375  \\\\\n",
      " 0.288137 & 0.641239 & 0.038019  & -0.0820253  \\\\\n",
      " 0.271496 & 0.599427 & 0.0479411 & -0.0886612  \\\\\n",
      " 0.261843 & 0.579628 & 0.0473643 & -0.110542   \\\\\n",
      " 0.255181 & 0.564687 & 0.037699  & -0.149698   \\\\\n",
      " 0.250305 & 0.557468 & 0.0346084 & -0.169822   \\\\\n",
      " 0.243017 & 0.545432 & 0.0392208 & -0.163043   \\\\\n",
      " 0.239634 & 0.540279 & 0.0398052 & -0.163908   \\\\\n",
      " 0.237118 & 0.536426 & 0.0403717 & -0.164838   \\\\\n",
      " 0.235022 & 0.533084 & 0.0409626 & -0.165758   \\\\\n",
      " 0.233204 & 0.530064 & 0.0415325 & -0.166684   \\\\\n",
      " 0.231584 & 0.527291 & 0.0420304 & -0.167627   \\\\\n",
      " 0.230119 & 0.524735 & 0.042427  & -0.168591   \\\\\n",
      " 0.228785 & 0.522377 & 0.042722  & -0.169573   \\\\\n",
      " 0.227563 & 0.520198 & 0.0429333 & -0.170561   \\\\\n",
      " 0.226439 & 0.518174 & 0.0430835 & -0.171539   \\\\\n",
      " 0.225401 & 0.516285 & 0.0431911 & -0.172484   \\\\\n",
      " 0.224441 & 0.510067 & 0.0450208 & -0.153957   \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.51187489]\n",
      " [0.47056227]\n",
      " [0.45747602]\n",
      " [0.44934395]\n",
      " [0.45292678]\n",
      " [0.45174389]\n",
      " [0.45189757]\n",
      " [0.44900394]\n",
      " [0.44555015]\n",
      " [0.44353706]\n",
      " [0.44203237]\n",
      " [0.44087287]\n",
      " [0.43995396]\n",
      " [0.43919566]\n",
      " [0.43854375]\n",
      " [0.4379679 ]\n",
      " [0.43745368]\n",
      " [0.43699381]\n",
      " [0.43658263]\n",
      " [0.44002375]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.5,0.8]\n",
    "NEWSIR(ar_coeff,sir_1_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d29ff5-2b04-41f0-9bbc-e471ca595231",
   "metadata": {},
   "source": [
    "## Use equal number in each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73049324-b848-473f-a5a2-a12157f40143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from 0 to Q double sum\n",
    "# from tabulate import tabulate\n",
    "# import numpy as np\n",
    "\n",
    "# def uu2(ar_coeff):\n",
    "#     import numpy as np\n",
    "#     from tabulate import tabulate\n",
    "    \n",
    "#     def sir_1_time_series1(X, y, s, num_slices, K):\n",
    "#         n_samples, n_features = X.shape\n",
    "#         V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "#         # Step 1: Sort the data by the response variable\n",
    "#         sorted_indices = np.argsort(y)\n",
    "#         X_sorted = X[sorted_indices]\n",
    "#         y_sorted = y[sorted_indices]\n",
    "#         # Step 2: Divide the data into slices\n",
    "#         slice_size = n_samples // num_slices\n",
    "#         ph_hat = slice_size / n_samples\n",
    "#         slices = []\n",
    "        \n",
    "#         for i in range(num_slices):\n",
    "#             start_idx = i * slice_size\n",
    "#             if i < num_slices - 1:\n",
    "#                 end_idx = (i + 1) * slice_size\n",
    "#             else:  # Last slice includes any remaining samples\n",
    "#                 end_idx = n_samples\n",
    "#             if start_idx - s > 0:\n",
    "#                 slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))\n",
    "#             else:\n",
    "#                 slices.append((X[sorted_indices[0:end_idx - s]], y_sorted[start_idx:end_idx]))\n",
    "        \n",
    "#         # Step 3: Compute the means of the predictors within each slice\n",
    "#         X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])\n",
    "        \n",
    "#         # Step 4: Center the predictor means\n",
    "#         X_centered = X_means - np.mean(X_means, axis=0)\n",
    "        \n",
    "#         V_hat = np.add(V_hat, ph_hat * np.matmul(X_centered.T, X_centered))\n",
    "\n",
    "#         # Check for NaN or inf values in V_hat\n",
    "#         if np.isnan(V_hat).any() or np.isinf(V_hat).any():\n",
    "#             # Handle NaN or inf values appropriately\n",
    "#             # For example, replace NaN values with 0 and inf values with a large number\n",
    "#             V_hat[np.isnan(V_hat)] = 0\n",
    "#             V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat\n",
    "    \n",
    "#     # Compute eigenvalues and eigenvectors\n",
    "#         eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "#         eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "#         K_index = np.argpartition(np.abs(eigenvalues), X.shape[1] - K) >= X.shape[1] - K\n",
    "#         K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "#         edr_est = K_largest_eigenvectors\n",
    "        \n",
    "#         return V_hat, edr_est, eigenvalues ** 2\n",
    "    \n",
    "#     num_N = 5\n",
    "#     n_obs = 1000\n",
    "#     noise = np.zeros((num_N, n_obs))\n",
    "#     n = 100\n",
    "#     H = 50\n",
    "#     P = 4\n",
    "#     K = 1\n",
    "#     S = 20\n",
    "#     hat = [np.zeros((P, 1)) for _ in range(S)]\n",
    "#     g = np.zeros((S, 1))\n",
    "#     # ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "#     n1 = 0\n",
    "#     l = 1  # Initialize `l` outside the loop\n",
    "#     while n1 < 100:\n",
    "#         for h in range(num_N):\n",
    "#             noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "#         ar_series = np.zeros((num_N, n_obs))\n",
    "#         for t in range(0, n_obs):\n",
    "#             ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]\n",
    "#             ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]\n",
    "#             ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]\n",
    "#             ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]\n",
    "#             ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]\n",
    "#         y = ar_series[4]\n",
    "#         X = np.concatenate([ar_series[i].reshape(-1, 1) for i in range(4)], axis=1)\n",
    "#         V = []\n",
    "#         for a in range(0, S + 1):\n",
    "#             M, _, _ = sir_1_time_series1(X, y, a, H, K)\n",
    "#             V.append(M)\n",
    "#         for q in range(1, S + 1):\n",
    "#             Q = np.zeros((P, P))\n",
    "#             phi = ar_coeff\n",
    "#             for j in range(P):\n",
    "#                 for k in range(P):\n",
    "#                     Q[j, k] = sum(sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, l)) for l in range(1, q + 1))\n",
    "#             eigenvalues1, eigenvectors1 = np.linalg.eig(Q)\n",
    "#             K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K\n",
    "#             K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "#             edr_est = K_largest_eigenvectors\n",
    "#             if edr_est[0] < 0:\n",
    "#                 edr_est = -edr_est\n",
    "#             edr_est = edr_est / np.linalg.norm(edr_est)\n",
    "#             hat[q - 1] += edr_est\n",
    "#             n1 += 1\n",
    "    \n",
    "#     for i in range(S):\n",
    "#         hat[i] = hat[i] / n\n",
    "#         g[i] = hat[i][0] / hat[i][1]\n",
    "#     array = np.array(hat)\n",
    "#     print(tabulate(array, tablefmt='latex'))\n",
    "#     print(g)\n",
    "\n",
    "# # Example usage\n",
    "# ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "# uu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51095720-5640-4dd2-a047-84097494b1ec",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.2,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a2d60-4ea5-4bd2-aea1-c9dca23ecfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coeff = [0.2,0.2,0.2,0.2]\n",
    "NEWSIR(ar_coeff, sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b351fef0-e7b6-4e7a-bbeb-f1c37ac84d7e",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "823ee4e3-d2e4-4536-9053-aa218671a448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0261015 & 0.0424326 & 0.00158348 & 0.000551446 \\\\\n",
      " 0.0260885 & 0.0424096 & 0.00172348 & 0.000470322 \\\\\n",
      " 0.0260789 & 0.0423897 & 0.00183951 & 0.000430528 \\\\\n",
      " 0.026071  & 0.0423736 & 0.00192171 & 0.000405332 \\\\\n",
      " 0.026064  & 0.0423599 & 0.00198741 & 0.000387352 \\\\\n",
      " 0.026058  & 0.0423481 & 0.0020394  & 0.000376529 \\\\\n",
      " 0.0260529 & 0.0423383 & 0.00208166 & 0.000368662 \\\\\n",
      " 0.0260486 & 0.0423299 & 0.00211583 & 0.000362479 \\\\\n",
      " 0.0260448 & 0.0423228 & 0.00214411 & 0.000357546 \\\\\n",
      " 0.0260416 & 0.0423167 & 0.00216775 & 0.000353525 \\\\\n",
      " 0.0260388 & 0.0423113 & 0.00218789 & 0.000350162 \\\\\n",
      " 0.0260364 & 0.0423067 & 0.00220526 & 0.000347249 \\\\\n",
      " 0.0260342 & 0.0423026 & 0.00222038 & 0.000344689 \\\\\n",
      " 0.0260323 & 0.042299  & 0.00223359 & 0.000342406 \\\\\n",
      " 0.0260306 & 0.0422959 & 0.00224522 & 0.000340366 \\\\\n",
      " 0.0260291 & 0.042293  & 0.00225552 & 0.00033852  \\\\\n",
      " 0.0260278 & 0.0422904 & 0.00226469 & 0.000336845 \\\\\n",
      " 0.0260266 & 0.0422881 & 0.00227292 & 0.000335322 \\\\\n",
      " 0.0260254 & 0.042286  & 0.00228034 & 0.000333933 \\\\\n",
      " 0.0260244 & 0.0422841 & 0.00228707 & 0.000332661 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.61512674]\n",
      " [0.61515486]\n",
      " [0.61521789]\n",
      " [0.61526367]\n",
      " [0.61530007]\n",
      " [0.61532859]\n",
      " [0.61535159]\n",
      " [0.61537029]\n",
      " [0.61538576]\n",
      " [0.6153988 ]\n",
      " [0.61540999]\n",
      " [0.61541968]\n",
      " [0.61542811]\n",
      " [0.61543549]\n",
      " [0.61544201]\n",
      " [0.6154478 ]\n",
      " [0.61545297]\n",
      " [0.61545762]\n",
      " [0.61546182]\n",
      " [0.61546562]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.8,0.8]\n",
    "NEWSIR(ar_coeff, sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe52e-3be9-4674-9ab2-7f2f473b55f1",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e458a68-7b29-453b-80be-0966839c9280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0242404 & 0.0432713 & 0.00111159 & 0.000576685 \\\\\n",
      " 0.0236342 & 0.043481  & 0.00131628 & 0.000700589 \\\\\n",
      " 0.0233527 & 0.0435416 & 0.00144546 & 0.000766658 \\\\\n",
      " 0.0231901 & 0.0435552 & 0.00153747 & 0.000827364 \\\\\n",
      " 0.0230834 & 0.0435519 & 0.00160634 & 0.000876656 \\\\\n",
      " 0.0230082 & 0.0435431 & 0.00165926 & 0.000912523 \\\\\n",
      " 0.0229521 & 0.0435329 & 0.00169987 & 0.000939351 \\\\\n",
      " 0.0229086 & 0.0435226 & 0.00173293 & 0.000960179 \\\\\n",
      " 0.0228738 & 0.043513  & 0.00176026 & 0.00097653  \\\\\n",
      " 0.0228455 & 0.0435043 & 0.00178293 & 0.000989822 \\\\\n",
      " 0.0228219 & 0.0434965 & 0.0018021  & 0.0010007   \\\\\n",
      " 0.0228021 & 0.0434895 & 0.00181849 & 0.00100975  \\\\\n",
      " 0.0227851 & 0.0434834 & 0.00183263 & 0.00101734  \\\\\n",
      " 0.0227704 & 0.0434778 & 0.00184493 & 0.00102382  \\\\\n",
      " 0.0227576 & 0.0434729 & 0.00185571 & 0.00102944  \\\\\n",
      " 0.0227464 & 0.0434684 & 0.00186521 & 0.00103435  \\\\\n",
      " 0.0227364 & 0.0434644 & 0.00187365 & 0.00103868  \\\\\n",
      " 0.0227275 & 0.0434607 & 0.0018812  & 0.00104252  \\\\\n",
      " 0.0227195 & 0.0434574 & 0.00188798 & 0.00104595  \\\\\n",
      " 0.0227122 & 0.0434544 & 0.00189413 & 0.00104905  \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.56019588]\n",
      " [0.54355199]\n",
      " [0.53633037]\n",
      " [0.53242969]\n",
      " [0.53002093]\n",
      " [0.52839909]\n",
      " [0.52723499]\n",
      " [0.52635975]\n",
      " [0.52567778]\n",
      " [0.52513157]\n",
      " [0.52468436]\n",
      " [0.52431145]\n",
      " [0.52399573]\n",
      " [0.523725  ]\n",
      " [0.5234903 ]\n",
      " [0.52328489]\n",
      " [0.5231036 ]\n",
      " [0.52294244]\n",
      " [0.52279822]\n",
      " [0.5226684 ]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.8,0.8]\n",
    "NEWSIR(ar_coeff, sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39aecd-ab82-4c3d-bf0c-e0fe47713e6b",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.5,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4c0ea3c2-9819-4dfc-81c1-bafd0a49bf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.021882  & 0.0445643 & 0.00130569 & -0.00163376 \\\\\n",
      " 0.0212254 & 0.0447941 & 0.00138485 & -0.00177341 \\\\\n",
      " 0.0209237 & 0.0448746 & 0.00141902 & -0.00188339 \\\\\n",
      " 0.0207576 & 0.0449085 & 0.00143795 & -0.00195949 \\\\\n",
      " 0.0206533 & 0.0449241 & 0.00144985 & -0.00201511 \\\\\n",
      " 0.020582  & 0.0449317 & 0.00145783 & -0.00205717 \\\\\n",
      " 0.0205302 & 0.0449356 & 0.00146355 & -0.00209    \\\\\n",
      " 0.0204908 & 0.0449374 & 0.00146785 & -0.00211639 \\\\\n",
      " 0.0204599 & 0.0449382 & 0.0014712  & -0.00213772 \\\\\n",
      " 0.0204349 & 0.0449385 & 0.00147389 & -0.00215533 \\\\\n",
      " 0.0204144 & 0.0449385 & 0.00147609 & -0.00216993 \\\\\n",
      " 0.0203972 & 0.0449383 & 0.00147792 & -0.00218226 \\\\\n",
      " 0.0203827 & 0.0449381 & 0.00147947 & -0.00219281 \\\\\n",
      " 0.0203701 & 0.0449378 & 0.0014808  & -0.00220195 \\\\\n",
      " 0.0203593 & 0.0449374 & 0.00148195 & -0.00220992 \\\\\n",
      " 0.0203497 & 0.0449371 & 0.00148296 & -0.00221693 \\\\\n",
      " 0.0203413 & 0.0449368 & 0.00148385 & -0.00222315 \\\\\n",
      " 0.0203338 & 0.0449365 & 0.00148464 & -0.00222868 \\\\\n",
      " 0.020327  & 0.0449362 & 0.00148535 & -0.00223365 \\\\\n",
      " 0.020321  & 0.0449359 & 0.00148599 & -0.00223812 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.49102136]\n",
      " [0.47384285]\n",
      " [0.46626931]\n",
      " [0.46222118]\n",
      " [0.459739  ]\n",
      " [0.45807292]\n",
      " [0.45688062]\n",
      " [0.45598521]\n",
      " [0.45528849]\n",
      " [0.45473085]\n",
      " [0.45427459]\n",
      " [0.45389435]\n",
      " [0.4535726 ]\n",
      " [0.45329679]\n",
      " [0.45305777]\n",
      " [0.45284865]\n",
      " [0.45266415]\n",
      " [0.45250018]\n",
      " [0.45235348]\n",
      " [0.45222147]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.5,0.8]\n",
    "NEWSIR(ar_coeff, sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeab50c-64b8-4f0c-9b8f-70c1a47fddd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# SIR for Time series new objective(Single sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a906ed-f527-471c-a8aa-e64222d5d1ea",
   "metadata": {},
   "source": [
    "If the objective is $$\\text{diag}(W^T V^{[1: Q]} W)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937da798-62b9-45b9-b3a2-13d283705e49",
   "metadata": {},
   "source": [
    "## Use equal slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eba56d3b-5959-490b-abe3-1afea5abc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 0 to Q single sum\n",
    "from tabulate import tabulate\n",
    "def NEWSIRsingle(ar_coeff):\n",
    "    num_N = 5\n",
    "    n_obs = 1000\n",
    "    noise = np.zeros((num_N, n_obs))\n",
    "    n = 100\n",
    "    H = 50\n",
    "    P = 4\n",
    "    K = 1\n",
    "    S = 20\n",
    "    hat = [np.zeros((P, 1)) for i in range(S)]\n",
    "    # ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "    g = np.zeros((S, 1))\n",
    "    for w in range(n):    \n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "        ar_series = np.zeros((num_N, n_obs))\n",
    "        for t in range(0, n_obs):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]\n",
    "            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]\n",
    "            ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]\n",
    "        y = ar_series[4]\n",
    "        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "        V = []\n",
    "        for a in range(0,S+1):\n",
    "            M, edr, lam1 = sir_1_time_series(X, y, a, H, K)\n",
    "            V.append(M)\n",
    "        for q in range(1, S+1):\n",
    "            Q = np.zeros((P, P))\n",
    "            phi = ar_coeff\n",
    "            for j in range(P):\n",
    "                for k in range(P):\n",
    "                    Q[j,k] = sum(phi[j]**a * V[a][j,k] * phi[k]**a for a in range(0,q)) #single sum\n",
    "            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)\n",
    "            K_index = np.argpartition(np.abs(eigenvalues1), P-K) >= P-K\n",
    "            K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "            edr_est =  K_largest_eigenvectors\n",
    "            if edr_est[0]<0:\n",
    "                edr_est = -edr_est\n",
    "            edr_est = edr_est/np.linalg.norm(edr_est)\n",
    "            hat[q-1] += edr_est       \n",
    "    for i in range(S):\n",
    "        hat[i] = hat[i]/n\n",
    "        g[i] = hat[i][0] / hat[i][1]\n",
    "    array = np.array(hat)\n",
    "    print(tabulate(array, tablefmt='latex'))\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fea24d-689b-4ec5-bbbc-d8536eeaea5a",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.2,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "311b845e-296b-4b11-b01c-fcc74c015f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.55235  & 0.831289 & -0.00652422 &  0.00176025 \\\\\n",
      " 0.554268 & 0.825945 & -0.0128939  & -0.00945419 \\\\\n",
      " 0.554282 & 0.825614 & -0.0131315  & -0.00992433 \\\\\n",
      " 0.554283 & 0.825601 & -0.0131409  & -0.00994309 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994384 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      " 0.554283 & 0.8256   & -0.0131413  & -0.00994387 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.6644504 ]\n",
      " [0.67107161]\n",
      " [0.67135713]\n",
      " [0.6713686 ]\n",
      " [0.67136905]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]\n",
      " [0.67136907]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.2,0.2]\n",
    "NEWSIRsingle(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567500d2-b296-468b-8068-9973be61ab32",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.2,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40504e31-5645-4b76-84f1-aaec87a624fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coeff = [0.2,0.2,0.8,0.8]\n",
    "NEWSIRsingle(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a4293-f6b6-4830-9910-ca66487a1979",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.5,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb08a0a-96d2-4a41-8819-382f66d8c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coeff = [0.2,0.5,0.5,0.8]\n",
    "NEWSIRsingle(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39416f-461f-45a4-92c2-64bdd2d6bf45",
   "metadata": {},
   "source": [
    "### ar_coeff = [0.2,0.5,0.8,0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8450ba-bc0b-49ef-a576-00481704bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coeff = [0.2,0.5,0.8,0.8]\n",
    "NEWSIRsingle(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1aed6-a689-46d6-a553-34ed9288b38e",
   "metadata": {},
   "source": [
    "## Use equal number in each slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "739897d3-fd39-4517-9dcc-2ed44ea96b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 0 to Q single sum\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "\n",
    "def uuu2(ar_coeff):\n",
    "    import numpy as np\n",
    "    from tabulate import tabulate\n",
    "    \n",
    "    def sir_1_time_series1(X, y, s, num_slices, K):\n",
    "        n_samples, n_features = X.shape\n",
    "        V_hat = np.zeros([X.shape[1], X.shape[1]])\n",
    "        # Step 1: Sort the data by the response variable\n",
    "        sorted_indices = np.argsort(y)\n",
    "        X_sorted = X[sorted_indices]\n",
    "        y_sorted = y[sorted_indices]\n",
    "        # Step 2: Divide the data into slices\n",
    "        slice_size = n_samples // num_slices\n",
    "        ph_hat = slice_size / n_samples\n",
    "        slices = []\n",
    "        \n",
    "        for i in range(num_slices):\n",
    "            start_idx = i * slice_size\n",
    "            if i < num_slices - 1:\n",
    "                end_idx = (i + 1) * slice_size\n",
    "            else:  # Last slice includes any remaining samples\n",
    "                end_idx = n_samples\n",
    "            if start_idx - s > 0:\n",
    "                slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))\n",
    "            else:\n",
    "                slices.append((X[sorted_indices[0:end_idx - s]], y_sorted[start_idx:end_idx]))\n",
    "        \n",
    "        # Step 3: Compute the means of the predictors within each slice\n",
    "        X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])\n",
    "        \n",
    "        # Step 4: Center the predictor means\n",
    "        X_centered = X_means - np.mean(X_means, axis=0)\n",
    "        \n",
    "        V_hat = np.add(V_hat, ph_hat * np.matmul(X_centered.T, X_centered))\n",
    "\n",
    "        #report wrong, there's  NaN or inf value in V_hat, so add this\n",
    "        # Check for NaN or inf values in V_hat\n",
    "        if np.isnan(V_hat).any() or np.isinf(V_hat).any():\n",
    "            # Handle NaN or inf values appropriately\n",
    "            # For example, replace NaN values with 0 and inf values with a large number\n",
    "            V_hat[np.isnan(V_hat)] = 0\n",
    "            V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(V_hat)\n",
    "        K_index = np.argpartition(np.abs(eigenvalues), X.shape[1] - K) >= X.shape[1] - K\n",
    "        K_largest_eigenvectors = eigenvectors[:, K_index]\n",
    "        edr_est = K_largest_eigenvectors\n",
    "        \n",
    "        return V_hat, edr_est, eigenvalues ** 2\n",
    "    \n",
    "    num_N = 5\n",
    "    n_obs = 1000\n",
    "    noise = np.zeros((num_N, n_obs))\n",
    "    n = 100\n",
    "    H = 50\n",
    "    P = 4\n",
    "    K = 1\n",
    "    S = 20\n",
    "    hat = [np.zeros((P, 1)) for _ in range(S)]\n",
    "    g = np.zeros((S, 1))\n",
    "    # ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "    n1 = 0\n",
    "    while n1 < 100:\n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "        ar_series = np.zeros((num_N, n_obs))\n",
    "        for t in range(0, n_obs):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]\n",
    "            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]\n",
    "            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]\n",
    "            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]\n",
    "        y = ar_series[4]\n",
    "        X = np.concatenate([ar_series[i].reshape(-1, 1) for i in range(4)], axis=1)\n",
    "        V = []\n",
    "        for a in range(0, S + 1):\n",
    "            M, _, _ = sir_1_time_series1(X, y, a, H, K)\n",
    "            V.append(M)\n",
    "        for q in range(1, S + 1):\n",
    "            Q = np.zeros((P, P))\n",
    "            phi = ar_coeff\n",
    "            for j in range(P):\n",
    "                for k in range(P):\n",
    "                    Q[j, k] = sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, q))\n",
    "            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)\n",
    "            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K\n",
    "            K_largest_eigenvectors = eigenvectors1[:, K_index]\n",
    "            edr_est = K_largest_eigenvectors\n",
    "            if edr_est[0] < 0:\n",
    "                edr_est = -edr_est\n",
    "            edr_est = edr_est / np.linalg.norm(edr_est)\n",
    "            hat[q - 1] += edr_est\n",
    "            n1 += 1\n",
    "    \n",
    "    for i in range(S):\n",
    "        hat[i] = hat[i] / n\n",
    "        g[i] = hat[i][0] / hat[i][1]\n",
    "    array = np.array(hat)\n",
    "    print(tabulate(array, tablefmt='latex'))\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a93f1-9daa-49fc-b390-c22972870a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coeff = [0.2, 0.2, 0.2, 0.2]\n",
    "uuu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19356add-f290-484b-89ba-a770e8cd6e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0233308 & 0.0439774 & 0.00148481 & -0.000284376 \\\\\n",
      " 0.0221942 & 0.0444542 & 0.00192512 & -0.000589303 \\\\\n",
      " 0.0219856 & 0.0444877 & 0.00210631 & -0.000800864 \\\\\n",
      " 0.0219211 & 0.0444724 & 0.0022261  & -0.00096454  \\\\\n",
      " 0.0219007 & 0.0444587 & 0.00228155 & -0.00103267  \\\\\n",
      " 0.0218916 & 0.0444483 & 0.00231401 & -0.00105981  \\\\\n",
      " 0.0218865 & 0.0444405 & 0.00233926 & -0.00107817  \\\\\n",
      " 0.0218835 & 0.0444349 & 0.00235877 & -0.00108906  \\\\\n",
      " 0.0218817 & 0.0444316 & 0.00236771 & -0.00109702  \\\\\n",
      " 0.0218806 & 0.0444295 & 0.00237299 & -0.00110342  \\\\\n",
      " 0.0218799 & 0.0444281 & 0.0023764  & -0.00110694  \\\\\n",
      " 0.0218794 & 0.0444272 & 0.00237852 & -0.00110875  \\\\\n",
      " 0.0218791 & 0.0444266 & 0.00238001 & -0.00111006  \\\\\n",
      " 0.0218789 & 0.0444262 & 0.00238104 & -0.00111085  \\\\\n",
      " 0.0218788 & 0.044426  & 0.00238187 & -0.00111133  \\\\\n",
      " 0.0218787 & 0.0444259 & 0.00238226 & -0.00111153  \\\\\n",
      " 0.0218787 & 0.0444258 & 0.0023826  & -0.00111165  \\\\\n",
      " 0.0218786 & 0.0444257 & 0.00238281 & -0.00111177  \\\\\n",
      " 0.0218786 & 0.0444256 & 0.00238299 & -0.00111189  \\\\\n",
      " 0.0218786 & 0.0444256 & 0.00238312 & -0.0011121   \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.53051831]\n",
      " [0.49925978]\n",
      " [0.49419655]\n",
      " [0.49291432]\n",
      " [0.49260783]\n",
      " [0.4925187 ]\n",
      " [0.4924912 ]\n",
      " [0.49248342]\n",
      " [0.4924806 ]\n",
      " [0.4924795 ]\n",
      " [0.49247878]\n",
      " [0.49247807]\n",
      " [0.49247767]\n",
      " [0.49247749]\n",
      " [0.4924774 ]\n",
      " [0.49247735]\n",
      " [0.49247728]\n",
      " [0.49247726]\n",
      " [0.49247725]\n",
      " [0.49247729]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.8,0.8]\n",
    "uuu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3cd0b89e-e1d5-45eb-912b-b62d90d07d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0281901 & 0.0407746 & 0.000983024 & -0.00229931 \\\\\n",
      " 0.0281302 & 0.0406785 & 0.00108717  & -0.00267873 \\\\\n",
      " 0.0280782 & 0.0405959 & 0.00119454  & -0.00292771 \\\\\n",
      " 0.0280312 & 0.0405244 & 0.00132309  & -0.00306977 \\\\\n",
      " 0.0279983 & 0.0404748 & 0.00143155  & -0.00316628 \\\\\n",
      " 0.0279731 & 0.0404372 & 0.00151865  & -0.00323207 \\\\\n",
      " 0.0279567 & 0.0404124 & 0.00156755  & -0.0032817  \\\\\n",
      " 0.0279465 & 0.0403974 & 0.00160851  & -0.0033108  \\\\\n",
      " 0.0279403 & 0.0403882 & 0.00163071  & -0.00332418 \\\\\n",
      " 0.0279365 & 0.0403826 & 0.00164288  & -0.00333221 \\\\\n",
      " 0.0279336 & 0.0403781 & 0.00165054  & -0.00333886 \\\\\n",
      " 0.0279319 & 0.0403755 & 0.00165522  & -0.00334191 \\\\\n",
      " 0.0279309 & 0.040374  & 0.0016585   & -0.0033438  \\\\\n",
      " 0.0279302 & 0.040373  & 0.0016607   & -0.00334503 \\\\\n",
      " 0.0279297 & 0.0403722 & 0.00166182  & -0.00334607 \\\\\n",
      " 0.0279294 & 0.0403718 & 0.00166244  & -0.00334668 \\\\\n",
      " 0.0279292 & 0.0403715 & 0.00166287  & -0.00334692 \\\\\n",
      " 0.0279291 & 0.0403713 & 0.0016633   & -0.00334719 \\\\\n",
      " 0.027929  & 0.0403712 & 0.00166347  & -0.0033474  \\\\\n",
      " 0.027929  & 0.0403711 & 0.00166347  & -0.00334765 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.69136287]\n",
      " [0.69152651]\n",
      " [0.69165107]\n",
      " [0.6917126 ]\n",
      " [0.6917463 ]\n",
      " [0.69176655]\n",
      " [0.69178432]\n",
      " [0.69178827]\n",
      " [0.69179286]\n",
      " [0.69179625]\n",
      " [0.69180003]\n",
      " [0.6918021 ]\n",
      " [0.69180302]\n",
      " [0.69180371]\n",
      " [0.69180441]\n",
      " [0.69180486]\n",
      " [0.6918051 ]\n",
      " [0.6918052 ]\n",
      " [0.69180528]\n",
      " [0.69180545]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.8,0.8]\n",
    "uuu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9728db29-71f3-4702-899b-0aa2b0a09a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0219593 & 0.04474   & -0.000577489 & -0.00238238 \\\\\n",
      " 0.0207578 & 0.0452431 & -0.000575591 & -0.00293161 \\\\\n",
      " 0.0205502 & 0.0453095 & -0.00058722  & -0.00322293 \\\\\n",
      " 0.0204969 & 0.0453141 & -0.000588143 & -0.00338837 \\\\\n",
      " 0.0204803 & 0.0453096 & -0.000589038 & -0.00347471 \\\\\n",
      " 0.0204743 & 0.0453051 & -0.000588676 & -0.00352741 \\\\\n",
      " 0.0204716 & 0.0453019 & -0.000588435 & -0.00355672 \\\\\n",
      " 0.0204698 & 0.0452991 & -0.000588162 & -0.00358144 \\\\\n",
      " 0.0204685 & 0.0452969 & -0.000587925 & -0.00359898 \\\\\n",
      " 0.0204677 & 0.0452956 & -0.000587791 & -0.00361018 \\\\\n",
      " 0.0204672 & 0.0452947 & -0.000587693 & -0.00361719 \\\\\n",
      " 0.020467  & 0.0452942 & -0.000587637 & -0.00362133 \\\\\n",
      " 0.0204668 & 0.0452938 & -0.000587589 & -0.00362406 \\\\\n",
      " 0.0204666 & 0.0452936 & -0.000587557 & -0.00362584 \\\\\n",
      " 0.0204666 & 0.0452935 & -0.000587538 & -0.00362686 \\\\\n",
      " 0.0204665 & 0.0452934 & -0.000587528 & -0.00362753 \\\\\n",
      " 0.0204665 & 0.0452933 & -0.000587521 & -0.00362791 \\\\\n",
      " 0.0204665 & 0.0452933 & -0.000587516 & -0.00362817 \\\\\n",
      " 0.0204665 & 0.0452933 & -0.000587514 & -0.00362834 \\\\\n",
      " 0.0204665 & 0.0452933 & -0.000587511 & -0.00362845 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.4908212 ]\n",
      " [0.45880658]\n",
      " [0.45355127]\n",
      " [0.45232988]\n",
      " [0.45200722]\n",
      " [0.45192081]\n",
      " [0.45189272]\n",
      " [0.45188019]\n",
      " [0.45187404]\n",
      " [0.4518703 ]\n",
      " [0.45186831]\n",
      " [0.45186715]\n",
      " [0.45186659]\n",
      " [0.45186622]\n",
      " [0.45186599]\n",
      " [0.45186581]\n",
      " [0.45186573]\n",
      " [0.45186567]\n",
      " [0.45186562]\n",
      " [0.45186559]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.5,0.5,0.8]\n",
    "uuu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d28abb88-0412-4ce5-a92a-ba726ae7a362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      " 0.0276621 & 0.0415087 & -0.00187225 & 0.00016349  \\\\\n",
      " 0.0276669 & 0.0415049 & -0.00186133 & 0.000174439 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174786 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174795 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      " 0.0276667 & 0.041505  & -0.0018606  & 0.000174796 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "[[0.66641746]\n",
      " [0.66659372]\n",
      " [0.66658635]\n",
      " [0.66658555]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]\n",
      " [0.66658554]]\n"
     ]
    }
   ],
   "source": [
    "ar_coeff = [0.2,0.2,0.2,0.2]\n",
    "uuu2(ar_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bc003-7818-4752-b1bf-eaafa4801c3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TSIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9b67efc8-575d-4731-b2b9-6d9cf260fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 0.2 can find edr direction\n",
    "def TSIR(ar_coeff, T):\n",
    "    n = 100\n",
    "    S = 12  \n",
    "    H = 50\n",
    "    K = 1\n",
    "    P = 4\n",
    "    num_N = 5\n",
    "    n_x = 2\n",
    "    n_obs = 1000\n",
    "    QW = np.zeros((P,4))\n",
    "    for w in range(n):\n",
    "        noise = np.zeros((num_N, n_obs))\n",
    "        ar_series = np.zeros((num_N, n_obs))\n",
    "        Newar_series = np.zeros((n_obs, num_N - 1))\n",
    "        M = np.zeros((num_N, n_obs))\n",
    "        X = []\n",
    "        num_dataframes = S\n",
    "        y = []\n",
    "        ar_seriesT1 = pd.DataFrame({})\n",
    "        ar_seriesT = pd.DataFrame({})\n",
    "        X1 = np.zeros((P,P))\n",
    "        for l in range(num_dataframes):    \n",
    "            dg = pd.DataFrame({})\n",
    "            X.append(dg)    \n",
    "        scaler = StandardScaler()  \n",
    "        lam = pd.DataFrame({})\n",
    "        # ar_coeff1 = 0.3  # Autoregressive coefficient\n",
    "        # ar_coeff2 = 0.7\n",
    "        B = []\n",
    "        edr1 = []\n",
    "        for h in range(num_N):\n",
    "            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise\n",
    "        # Generate the additive AR time series\n",
    "        for t in range(0, n_obs):\n",
    "            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]\n",
    "            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]\n",
    "            ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]\n",
    "            ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]\n",
    "            ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]\n",
    "        y = ar_series[4]\n",
    "        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)\n",
    "        X = scaler.fit_transform(X)\n",
    "        # for t in range(0, n_obs):\n",
    "        #     ar_seriesT1 = pd.concat([ar_seriesT1, pd.DataFrame([[ar_series[0][t], ar_series[1][t], ar_series[2][t], ar_series[3][t]]])], axis = 0, ignore_index=True)\n",
    "        #     X1 = X1 + ar_seriesT1.values[t].reshape(-1, 1) @ ar_seriesT1.values[t].reshape(-1, 1).T\n",
    "        # X1 = 1/n_obs*X1    \n",
    "        # eigenvalues, eigenvectors = np.linalg.eig(X1)\n",
    "        # # Compute reciprocal square root of eigenvalues\n",
    "        # recip_sqrt_eigenvalues = np.diag(1 / np.sqrt(eigenvalues))\n",
    "        # # Reconstruct the matrix\n",
    "        # A_neg_half = eigenvectors @ recip_sqrt_eigenvalues @ np.linalg.inv(eigenvectors)\n",
    "        # for t in range(0, n_obs):\n",
    "        #     # M = np.array([Newar_series[i][t] for i in range(num_N - 1)])\n",
    "        #     Newar_series[t] = A_neg_half @ ar_seriesT1.values[t]\n",
    "        for j in range(1,S + 1):\n",
    "            # for T in range(1,n_obs + 1):   \n",
    "            #     if T + j >= (n_obs):\n",
    "            #         break\n",
    "            #     else:\n",
    "                    # y[j-1] = pd.concat([y[j-1], pd.Series([ar_series[4][T+j]])], ignore_index=True)\n",
    "                    # M = pd.DataFrame({})\n",
    "                    # for i in range(0,5):\n",
    "                    #     M[i] = ar_series[i][T]\n",
    "            # ar_seriesT = pd.DataFrame([[Newar_series[T][0], Newar_series[T][1], Newar_series[T][2], Newar_series[T][3]]])    \n",
    "            V, edr, lam1 = T(X, y, j, H, K=K)\n",
    "            B.append(V)\n",
    "            edr1.append(edr)\n",
    "        from scipy.stats import ortho_group\n",
    "        def create_random_orthogonal_matrix(rows, cols):\n",
    "            # Create a random matrix\n",
    "            random_matrix = np.random.rand(rows, cols)    \n",
    "            # Perform QR decomposition\n",
    "            q, r = np.linalg.qr(random_matrix)    \n",
    "            return q\n",
    "        rows = 4\n",
    "        cols = 4\n",
    "        P1 = create_random_orthogonal_matrix(rows, cols)\n",
    "        #W_i\n",
    "        Gam = np.zeros((4,4))\n",
    "        r = np.zeros((4,4))\n",
    "        n1 = 0\n",
    "        while n1<=10000:\n",
    "            for i in range(K):\n",
    "                for k in range(S):\n",
    "                    Gam[i] += (P1[i] @ B[k] @ P1[i]) * B[k] @ P1[i] \n",
    "            U, S1, VT = np.linalg.svd(Gam)\n",
    "            QQ = U @ VT\n",
    "            P1 = QQ\n",
    "            n1 = n1 + 1 \n",
    "        for i in range(len(QQ)):\n",
    "            if QQ[i][0]<0:\n",
    "                QQ[i] = -QQ[i]\n",
    "        QQ = QQ/np.linalg.norm(QQ)\n",
    "        # sum(lam.apply(find_largest, axis=1).values[i]*edr1[i] for i in range(len(edr1)))\n",
    "        QW += QQ\n",
    "    \n",
    "        # LAM = pd.DataFrame({})\n",
    "        # for j in range(1, S + 1):\n",
    "        #     for i in range(4):\n",
    "        #         LAM1= (QQ[i] @ B[j-1] @ QQ[i].T)**2\n",
    "        #         LAM = pd.concat([LAM, pd.DataFrame([LAM1])], ignore_index=True)\n",
    "        # total_sum = LAM.sum().sum()\n",
    "        # LAM = LAM/total_sum\n",
    "        # reshaped_data = LAM.values.reshape(S, 4)\n",
    "        # # Convert reshaped data back to DataFrame\n",
    "        # LAM = pd.DataFrame(reshaped_data)\n",
    "        # sorted_columns = LAM.sum().sort_values().index\n",
    "        # Reorder DataFrame columns based on sorted column names\n",
    "        # LAM = LAM[sorted_columns]\n",
    "        \n",
    "    QW = QW/n\n",
    "    \n",
    "    return QW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9e7f7c7-0218-463c-bb7a-14cccbdc34e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.74463098e-01,  4.16778804e-01, -3.52710953e-04,\n",
       "         2.12060850e-03],\n",
       "       [ 4.17685428e-01, -2.73863926e-01,  3.77812782e-04,\n",
       "        -1.52171407e-03],\n",
       "       [ 0.00000000e+00,  5.02152914e-04,  4.99345206e-01,\n",
       "         2.06553633e-05],\n",
       "       [ 0.00000000e+00, -2.61130959e-03,  2.06553633e-05,\n",
       "         4.99567541e-01]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSIR([0.2,0.2], sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d4280d19-b917-4846-aeec-146c9ad6fea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22438364e-01,  4.82752552e-01,  3.08550664e-03,\n",
       "        -2.66403375e-03],\n",
       "       [ 4.84052887e-01, -1.22117915e-01, -8.74153785e-04,\n",
       "         7.29552118e-04],\n",
       "       [ 0.00000000e+00, -3.21321710e-03,  4.98983577e-01,\n",
       "         2.85369021e-05],\n",
       "       [ 0.00000000e+00,  2.76436169e-03,  2.85369021e-05,\n",
       "         4.99675328e-01]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSIR([0.2,0.8], sir_1_time_series1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "923e4b78-d262-4e7f-ab99-0e1d8b60287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21742315, -0.00966039,  0.01254573,  0.03445188],\n",
       "       [ 0.42838298, -0.00425975, -0.00105679, -0.0245955 ],\n",
       "       [ 0.        ,  0.03597271,  0.4086829 ,  0.0046153 ],\n",
       "       [ 0.        ,  0.02906866,  0.0046153 ,  0.36736469]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSIR([0.2,0.2], sir_1_time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6180dc2e-7bc9-4119-95b3-2f8908aee8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.33343568e-01, -2.49433031e-03, -6.40901368e-03,\n",
       "         3.76527795e-02],\n",
       "       [ 4.12871385e-01, -3.52583083e-03,  4.42063147e-03,\n",
       "        -7.69900231e-04],\n",
       "       [ 0.00000000e+00, -4.12862559e-02,  3.69667869e-01,\n",
       "         2.30828716e-03],\n",
       "       [ 0.00000000e+00,  7.89737522e-05,  2.30828716e-03,\n",
       "         3.42968195e-01]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TSIR([0.2,0.8], sir_1_time_series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
