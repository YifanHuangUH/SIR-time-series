# -*- coding: utf-8 -*-
"""Summary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15vg9mIUSpwn2zD5-KGVkrUj151rwweJW

# Defined functions
"""

import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.arima_process import ArmaProcess
from matplotlib import pyplot as plt
from scipy.linalg import sqrtm
def euclidean_distance(point1, point2):
    return np.sqrt(np.sum((point1 - point2)**2))
# equal slice
def sir_1(X, y, H, K):
    Z = X-np.mean(X, axis=0)
    width = (np.max(y) - np.min(y)) / H
    V_hat = np.zeros([X.shape[1], X.shape[1]])
    for h in range(H):
        h_index = np.logical_and(np.min(y)+h*width <= y, y < np.min(y)+(h+1)*width)
        ph_hat = np.mean(h_index)
        if ph_hat == 0:
            continue
        mh = np.mean(Z[h_index, :], axis=0)
        V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))
    eigenvalues, eigenvectors = np.linalg.eig(V_hat)
    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
    K_largest_eigenvectors = eigenvectors[:, K_index]
    edr_est =  K_largest_eigenvectors
    return  edr_est

# equal slice
def sir_1_time_series(X, y, s, H, K):
    from collections import deque
    def move_left(queue, steps):
        # Length of the queue
        n = len(queue)
        # Create a new list filled with 0
        new_queue = [0] * n

        # Move elements to the left by the specified number of steps
        for i in range(n):
            new_position = i - steps
            if new_position >= 0:
                new_queue[new_position] = queue[i]

        return new_queue

    Z = X-np.mean(X, axis=0)
    width = (np.max(y) - np.min(y)) / H
    V_hat = np.zeros([X.shape[1], X.shape[1]])
    for h in range(H):
        h_index = np.logical_and(np.min(y)+h*width <= y, y < np.min(y)+(h+1)*width)
        # queue moves s locations backward
        h_index = move_left(h_index, s)
        # h_index = h_index[len(y) - X.shape[0]:]
        # h_index = h_index[:X.shape[0]]
        ph_hat = np.mean(h_index)
        if ph_hat == 0:
            continue
        # if np.all((h_index == 1) < X.shape[0]):
        mh = np.mean(Z[h_index, :], axis=0)
        V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))
        # else:
        #     h_index = h_index
        #     mh = np.mean(Z[h_index, :], axis=0)
        #     V_hat = np.add(V_hat,ph_hat * np.matmul(mh[:, np.newaxis], mh[np.newaxis, :]))
    eigenvalues, eigenvectors = np.linalg.eig(V_hat)
    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
    K_largest_eigenvectors = eigenvectors[:, K_index]
    edr_est =  K_largest_eigenvectors
    return  (V_hat, edr_est, eigenvalues ** 2)

#equal number in each slice
#queue structure
def sir_1_time_series1(X, y, s, num_slices, K):
    n_samples, n_features = X.shape
    V_hat = np.zeros([X.shape[1], X.shape[1]])
    # Step 1: Sort the data by the response variable
    sorted_indices = np.argsort(y)
    # sorted_indices1 = np.argsort(y[0:X.shape[0]])
    # if len(sorted_indices) > X.shape[0]:
    #     X_sorted = X[0:X.shape[0]][sorted_indices1]
    # else:
    X_sorted = X[sorted_indices]
    y_sorted = y[sorted_indices]
    # Step 2: Divide the data into slices
    slice_size = n_samples // num_slices
    ph_hat = slice_size/n_samples
    slices = []
    # original indices are the order of t, the sorted indices are the order of y from its size
    # argsort gives the indices for the order of y from its size
    for i in range(num_slices):
        start_idx = i * slice_size
        if i < num_slices - 1:
            end_idx = (i + 1) * slice_size
        else:  # Last slice includes any remaining samples
            end_idx = n_samples
        if start_idx-s>0:
            slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))
        else:
            slices.append((X[sorted_indices[0:end_idx-s]], y_sorted[start_idx:end_idx]))

    # Step 3: Compute the means of the predictors within each slice
    X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

    # Step 4: Center the predictor means
    X_centered = X_means - np.mean(X_means, axis=0)
    V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))
    # Check for NaN or inf values in V_hat
    if np.isnan(V_hat).any() or np.isinf(V_hat).any():
        # Handle NaN or inf values appropriately
        # For example, replace NaN values with 0 and inf values with a large number
        V_hat[np.isnan(V_hat)] = 0
        V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat

    eigenvalues, eigenvectors = np.linalg.eig(V_hat)
    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
    K_largest_eigenvectors = eigenvectors[:, K_index]
    edr_est =  K_largest_eigenvectors

    return (V_hat, edr_est, eigenvalues ** 2)

# def sir_1_time_series2(X, y, num_slices, K):
#     n_samples, n_features = X.shape
#     V_hat = np.zeros([X.shape[1], X.shape[1]])
#     # Step 1: Sort the data by the response variable
#     sorted_indices = np.argsort(y)
#     # sorted_indices1 = np.argsort(y[0:X.shape[0]])
#     # if len(sorted_indices) > X.shape[0]:
#     #     X_sorted = X[0:X.shape[0]][sorted_indices1]
#     # else:
#     X_sorted = X[sorted_indices]
#     y_sorted = y[sorted_indices]
#     # Step 2: Divide the data into slices
#     slice_size = n_samples // num_slices
#     ph_hat = slice_size/n_samples
#     slices = []
#     # original indices are the order of t, the sorted indices are the order of y from its size
#     # argsort gives the indices for the order of y from its size
#     for i in range(num_slices):
#         start_idx = i * slice_size
#         if i < num_slices - 1:
#             end_idx = (i + 1) * slice_size
#         else:  # Last slice includes any remaining samples
#             end_idx = n_samples
#         if start_idx-s>0:
#             slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))
#         else:
#             slices.append((X[sorted_indices[0:end_idx-s]], y_sorted[start_idx:end_idx]))

#     # Step 3: Compute the means of the predictors within each slice
#     X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

#     # Step 4: Center the predictor means
#     X_centered = X_means - np.mean(X_means, axis=0)
#     V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))
#     # Check for NaN or inf values in V_hat
#     if np.isnan(V_hat).any() or np.isinf(V_hat).any():
#         # Handle NaN or inf values appropriately
#         # For example, replace NaN values with 0 and inf values with a large number
#         V_hat[np.isnan(V_hat)] = 0
#         V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat

#     eigenvalues, eigenvectors = np.linalg.eig(V_hat)
#     K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
#     K_largest_eigenvectors = eigenvectors[:, K_index]
#     edr_est =  K_largest_eigenvectors

#     return (V_hat, edr_est, eigenvalues ** 2)

import numpy as np
X = [1,4,5,3,7]
sort1 = np.argsort(X)
sort1

#equal number in each slice for SIR
def sir_11(X, y, num_slices, K):
    n_samples, n_features = X.shape
    V_hat = np.zeros([X.shape[1], X.shape[1]])
    # Step 1: Sort the data by the response variable
    sorted_indices = np.argsort(y)
    X_sorted = X[sorted_indices]
    y_sorted = y[sorted_indices]

    # Step 2: Divide the data into slices
    slice_size = n_samples // num_slices
    ph_hat = slice_size/n_samples
    slices = []

    for i in range(num_slices):
        start_idx = i * slice_size
        if i < num_slices - 1:
            end_idx = (i + 1) * slice_size
        else:  # Last slice includes any remaining samples
            end_idx = n_samples
        slices.append((X_sorted[start_idx:end_idx], y_sorted[start_idx:end_idx]))

    # Step 3: Compute the means of the predictors within each slice
    X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

    # Step 4: Center the predictor means
    X_centered = X_means - np.mean(X_means, axis=0)

    V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))
    eigenvalues, eigenvectors = np.linalg.eig(V_hat)
    K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
    K_largest_eigenvectors = eigenvectors[:, K_index]
    edr_est =  K_largest_eigenvectors

    return edr_est

"""### Above are functions for SIR, SIR different output(V_hat, edr_est, eigenvalues ** 2 and edr_est), SIR for equal number in each slice and SIR with equal slice.

# SIR
## The results are shown by 1. directions 2. ratio between first two coefficients

For model
\begin{align}
   & y_t = 2z_{1,t-1} + 3z_{2,t-1} +\epsilon_t
    \label{model:time series 2}
\end{align}
with four components $z_1 \sim \text{AR}(1)$ with $\phi_1$, $z_2 \sim \text{AR}(1)$ with $\phi_2$, $z_3 \sim \text{ARMA}(1, 1)$ with $\phi = 0.3$ and $\theta = 0.4$ and $z_4 \sim \text{MA}(1)$ with $\theta = -0.4$. Dimension $p = 4$. $K = 1$.

## Results under Sir_1(equal slice) and Sir_11(equal number in each slice)

### coefficient 0.2 0.2, correct direction
"""

#0.2 0.2 correct direction
num_N = 5
n_obs = 10000
ar_coeff1 = 0.2
ar_coeff2 = 0.2
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
Hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]
        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""### coefficient 0.2 0.8, incorrect direction"""

#0.2 0.8 incorrect direction
num_N = 5
n_obs = 10000
ar_coeff1 = 0.2
ar_coeff2 = 0.8
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
Hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]
        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""### coefficient 0.8 0.8, correct direction"""

#0.8 0.8 correct direction
num_N = 5
n_obs = 10000
ar_coeff1 = 0.8
ar_coeff2 = 0.8
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
Hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff1 * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff2 * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]
        ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")



"""## SIR for model
\begin{align}
   & y_t = 2z_{1,t} + 3z_{2,t} +\epsilon_t
\end{align}
with four components $z_i \sim \text{AR}(1)$ with $\phi_i, i = 1, \ldots, 4$. I denote the model as NO(0.2, 0.2, 0.2, 0.2) if $\phi_i = 0.2, i = 1, \ldots, 4$. $K = 1$.

### ar_coeff = [0.2, 0.2, 0.2, 0.2]
"""

#ar_coeff = [0.2, 0.2, 0.2, 0.2]
num_N = 5
n_obs = 10000
ar_coeff = [0.2, 0.2, 0.2, 0.2]
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H=H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""### ar_coeff = [0.2, 0.2, 0.8, 0.8]"""

#ar_coeff = [0.2, 0.2, 0.8, 0.8]
num_N = 5
n_obs = 10000
ar_coeff = [0.2, 0.2, 0.8, 0.8]
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H=H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""### ar_coeff = [0.2, 0.5, 0.8, 0.8]"""

#ar_coeff = [0.2, 0.5, 0.8, 0.8]
num_N = 5
n_obs = 10000
ar_coeff = [0.2, 0.5, 0.8, 0.8]
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H=H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""### ar_coeff = [0.2, 0.5, 0.5, 0.8]"""

#ar_coeff = [0.2, 0.5, 0.5, 0.8]
num_N = 5
n_obs = 10000
ar_coeff = [0.2, 0.5, 0.5, 0.8]
noise = np.zeros((num_N, n_obs))
n = 100
H = 50
K = 1
hat = 0
for a in range(n):
    for h in range(num_N):
        noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
    ar_series = np.zeros((num_N, n_obs))
    for t in range(0, n_obs):
        ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
        ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
        ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
        ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
        ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
    y = ar_series[4]
    X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
    sir_1_result = sir_1(X, y, H=H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_1_result[0]<0:
        sir_1_result = -sir_1_result
    sir_1_result = sir_1_result/np.linalg.norm(sir_1_result)
    hat = hat + sir_1_result
    sir_11_result = sir_11(X, y, H, K=K)
    #after adding normalization and modifying the first coefficient
    if sir_11_result[0]<0:
        sir_11_result = -sir_11_result
    sir_11_result = sir_11_result/np.linalg.norm(sir_11_result)
    Hat = Hat + sir_11_result
print(f"Equal slice direction:{hat/n}")
hat1 = hat/n
print(f"Equal slice ratio:{hat1[0]/hat1[1]}")
print(f"Equal number in each slice direction:{Hat/n}")
Hat1 = Hat/n
print(f"Equal number in each slice ratio:{Hat1[0]/Hat1[1]}")

"""# SIR for Time series new objective(Double sum)
## The results are shown by 1. directions 2. ratio between first two coefficients

Code below is for this derivation. When taking the sum across all $q$, it is easily seen that
$$
\sum_q diag\left[ (\mathbf{W} \circ \Phi^{\circ q})^T Cov(\mathbb{E}(\mathbf{X}_{t-q}|y) (\mathbf{W} \circ\Phi^{\circ q})\right]=diag(\mathbf{W}^T\sum_q \mathbf{V}^{[1:q]}\mathbf{W}),
$$
where
$$
\mathbf{V}^{[1:Q]}_{jk}=\sum_{q=1}^Q\mathbf{V}^q_{jk}\cdot \phi_j^q\cdot \phi_k^q.
$$
I'm wondering, is it $\text{diag}(W^T V^{[1: q]} W)$, not $\text{diag}(\mathbf{W}^T\sum_q \mathbf{V}^{[1:q]}\mathbf{W}).$

## Use equal slice
"""

#from 0 to Q double sum
from tabulate import tabulate
def NEWSIR(ar_coeff, T, n_obs, S):
    num_N = 5
    # n_obs = 10000
    noise = np.zeros((num_N, n_obs))
    n = 100
    H = 50
    P = 4
    K = 1
    # S = 20
    hat = [np.zeros((P, 1)) for i in range(S)]
    # ar_coeff = [0.2, 0.2, 0.2, 0.2]
    g = np.zeros((S, 1))
    for w in range(n):
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
        ar_series = np.zeros((num_N, n_obs))
        for t in range(0, n_obs):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
            ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
        y = ar_series[4]
        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
        V = []
        for a in range(0,S+1):
            M, edr, lam1 = T(X, y, a, H, K)
            V.append(M)
        for q in range(1, S+1):
            Q = np.zeros((P, P))
            phi = ar_coeff
            for j in range(P):
                for k in range(P):
                    Q[j,k] = sum(sum(phi[j]**a * V[a][j,k] * phi[k]**a for a in range(0,l)) for l in range(1, q+1)) #double sum
            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
            K_index = np.argpartition(np.abs(eigenvalues1), P-K) >= P-K
            K_largest_eigenvectors = eigenvectors1[:, K_index]
            edr_est =  K_largest_eigenvectors
            if edr_est[0]<0:
                edr_est = -edr_est
            edr_est = edr_est/np.linalg.norm(edr_est)
            hat[q-1] += edr_est
    for i in range(S):
        hat[i] = hat[i]/n
        g[i] = hat[i][0] / hat[i][1]
    array = np.array(hat)
    print(tabulate(array, tablefmt='latex'))
    print(g)

"""### ar_coeff = [0.2,0.2,0.2,0.2]"""

ar_coeff = [0.2,0.2,0.2,0.2]
NEWSIR(ar_coeff, sir_1_time_series, 10000, 50)

"""### ar_coeff = [0.2,0.2,0.8,0.8]"""

ar_coeff = [0.2,0.2,0.8,0.8]
NEWSIR(ar_coeff,sir_1_time_series, 10000, 10)

"""### ar_coeff = [0.2,0.5,0.8,0.8]"""

ar_coeff = [0.2,0.5,0.8,0.8]
NEWSIR(ar_coeff,sir_1_time_series, 10000, 50)

"""### ar_coeff = [0.2,0.5,0.5,0.8]"""

ar_coeff = [0.2,0.5,0.5,0.8]
NEWSIR(ar_coeff,sir_1_time_series, 10000, 50)

"""## Use equal number in each slice"""

# #from 0 to Q double sum
# from tabulate import tabulate
# import numpy as np

# def uu2(ar_coeff):
#     import numpy as np
#     from tabulate import tabulate

#     def sir_1_time_series1(X, y, s, num_slices, K):
#         n_samples, n_features = X.shape
#         V_hat = np.zeros([X.shape[1], X.shape[1]])
#         # Step 1: Sort the data by the response variable
#         sorted_indices = np.argsort(y)
#         X_sorted = X[sorted_indices]
#         y_sorted = y[sorted_indices]
#         # Step 2: Divide the data into slices
#         slice_size = n_samples // num_slices
#         ph_hat = slice_size / n_samples
#         slices = []

#         for i in range(num_slices):
#             start_idx = i * slice_size
#             if i < num_slices - 1:
#                 end_idx = (i + 1) * slice_size
#             else:  # Last slice includes any remaining samples
#                 end_idx = n_samples
#             if start_idx - s > 0:
#                 slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))
#             else:
#                 slices.append((X[sorted_indices[0:end_idx - s]], y_sorted[start_idx:end_idx]))

#         # Step 3: Compute the means of the predictors within each slice
#         X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

#         # Step 4: Center the predictor means
#         X_centered = X_means - np.mean(X_means, axis=0)

#         V_hat = np.add(V_hat, ph_hat * np.matmul(X_centered.T, X_centered))

#         # Check for NaN or inf values in V_hat
#         if np.isnan(V_hat).any() or np.isinf(V_hat).any():
#             # Handle NaN or inf values appropriately
#             # For example, replace NaN values with 0 and inf values with a large number
#             V_hat[np.isnan(V_hat)] = 0
#             V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat

#     # Compute eigenvalues and eigenvectors
#         eigenvalues, eigenvectors = np.linalg.eig(V_hat)
#         eigenvalues, eigenvectors = np.linalg.eig(V_hat)
#         K_index = np.argpartition(np.abs(eigenvalues), X.shape[1] - K) >= X.shape[1] - K
#         K_largest_eigenvectors = eigenvectors[:, K_index]
#         edr_est = K_largest_eigenvectors

#         return V_hat, edr_est, eigenvalues ** 2

#     num_N = 5
#     n_obs = 1000
#     noise = np.zeros((num_N, n_obs))
#     n = 100
#     H = 50
#     P = 4
#     K = 1
#     S = 20
#     hat = [np.zeros((P, 1)) for _ in range(S)]
#     g = np.zeros((S, 1))
#     # ar_coeff = [0.2, 0.2, 0.2, 0.2]
#     n1 = 0
#     l = 1  # Initialize `l` outside the loop
#     while n1 < 100:
#         for h in range(num_N):
#             noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
#         ar_series = np.zeros((num_N, n_obs))
#         for t in range(0, n_obs):
#             ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]
#             ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]
#             ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]
#             ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]
#             ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]
#         y = ar_series[4]
#         X = np.concatenate([ar_series[i].reshape(-1, 1) for i in range(4)], axis=1)
#         V = []
#         for a in range(0, S + 1):
#             M, _, _ = sir_1_time_series1(X, y, a, H, K)
#             V.append(M)
#         for q in range(1, S + 1):
#             Q = np.zeros((P, P))
#             phi = ar_coeff
#             for j in range(P):
#                 for k in range(P):
#                     Q[j, k] = sum(sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, l)) for l in range(1, q + 1))
#             eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
#             K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K
#             K_largest_eigenvectors = eigenvectors1[:, K_index]
#             edr_est = K_largest_eigenvectors
#             if edr_est[0] < 0:
#                 edr_est = -edr_est
#             edr_est = edr_est / np.linalg.norm(edr_est)
#             hat[q - 1] += edr_est
#             n1 += 1

#     for i in range(S):
#         hat[i] = hat[i] / n
#         g[i] = hat[i][0] / hat[i][1]
#     array = np.array(hat)
#     print(tabulate(array, tablefmt='latex'))
#     print(g)

# # Example usage
# ar_coeff = [0.2, 0.2, 0.2, 0.2]
# uu2(ar_coeff)

#from 0 to Q double sum
from tabulate import tabulate
import numpy as np

def uu2(ar_coeff):
    import numpy as np
    from tabulate import tabulate

    def sir_1_time_series2(X, y, num_slices, K):
        n_samples, n_features = X.shape
        V_hat = np.zeros([X.shape[1], X.shape[1]])
        # Step 1: Sort the data by the response variable
        sorted_indices = np.argsort(y)
        X_sorted = X[sorted_indices]
        y_sorted = y[sorted_indices]

        # Step 2: Divide the data into slices
        slice_size = n_samples // num_slices
        ph_hat = slice_size/n_samples
        slices = []

        for i in range(num_slices):
            start_idx = i * slice_size
            if i < num_slices - 1:
                end_idx = (i + 1) * slice_size
            else:  # Last slice includes any remaining samples
                end_idx = n_samples
            slices.append((X_sorted[start_idx:end_idx], y_sorted[start_idx:end_idx]))

        # Step 3: Compute the means of the predictors within each slice
        X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

        # Step 4: Center the predictor means
        X_centered = X_means - np.mean(X_means, axis=0)

        V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))
        eigenvalues, eigenvectors = np.linalg.eig(V_hat)
        K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
        K_largest_eigenvectors = eigenvectors[:, K_index]
        edr_est =  K_largest_eigenvectors

        return V_hat, edr_est, eigenvalues ** 2

    num_N = 5
    n_obs = 1000
    S = 20
    noise = np.zeros((num_N, n_obs+S))
    n = 100
    H = 50
    P = 4
    K = 1

    hat = [np.zeros((P, 1)) for _ in range(S)]
    g = np.zeros((S, 1))
    # ar_coeff = [0.2, 0.2, 0.2, 0.2]
    n1 = 0
    l = 1  # Initialize `l` outside the loop
    while n1 < 100:
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=(n_obs+S))  # Normally distributed noise
        ar_series = np.zeros((num_N, n_obs+S))
        for t in range(0, n_obs+S):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]
            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]
            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]
            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]
        y = ar_series[4][S:n_obs+S]
        X = np.concatenate([ar_series[i][0:n_obs].reshape(-1, 1) for i in range(4)], axis=1)
        V = []
        for a in range(0, S + 1):
            M, _, _ = sir_1_time_series2(X, y, H, K)
            V.append(M)
        for q in range(1, S + 1):
            Q = np.zeros((P, P))
            phi = ar_coeff
            for j in range(P):
                for k in range(P):
                    Q[j, k] = sum(sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, l)) for l in range(1, q + 1))
            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K
            K_largest_eigenvectors = eigenvectors1[:, K_index]
            edr_est = K_largest_eigenvectors
            if edr_est[0] < 0:
                edr_est = -edr_est
            edr_est = edr_est / np.linalg.norm(edr_est)
            hat[q - 1] += edr_est
            n1 += 1

    for i in range(S):
        hat[i] = hat[i] / n
        g[i] = hat[i][0] / hat[i][1]
    array = np.array(hat)
    print(tabulate(array, tablefmt='latex'))
    print(g)

# Example usage
ar_coeff = [0.2, 0.2, 0.2, 0.2]
uu2(ar_coeff)



#from 0 to Q double sum
from tabulate import tabulate
import numpy as np

def uu2(ar_coeff):
    import numpy as np
    from tabulate import tabulate

    def sir_1_time_series2(X, y, num_slices, K):
        n_samples, n_features = X.shape
        V_hat = np.zeros([X.shape[1], X.shape[1]])
        # Step 1: Sort the data by the response variable
        sorted_indices = np.argsort(y)
        X_sorted = X[sorted_indices]
        y_sorted = y[sorted_indices]

        # Step 2: Divide the data into slices
        slice_size = n_samples // num_slices
        ph_hat = slice_size/n_samples
        slices = []

        for i in range(num_slices):
            start_idx = i * slice_size
            if i < num_slices - 1:
                end_idx = (i + 1) * slice_size
            else:  # Last slice includes any remaining samples
                end_idx = n_samples
            slices.append((X_sorted[start_idx:end_idx], y_sorted[start_idx:end_idx]))

        # Step 3: Compute the means of the predictors within each slice
        X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

        # Step 4: Center the predictor means
        X_centered = X_means - np.mean(X_means, axis=0)

        V_hat = np.add(V_hat,ph_hat * np.matmul(X_centered.T, X_centered))
        eigenvalues, eigenvectors = np.linalg.eig(V_hat)
        K_index = np.argpartition(np.abs(eigenvalues), X.shape[1]-K) >= X.shape[1]-K
        K_largest_eigenvectors = eigenvectors[:, K_index]
        edr_est =  K_largest_eigenvectors

        return V_hat, edr_est, eigenvalues ** 2

    num_N = 5
    n_obs = 10000
    S = 21
    noise = np.zeros((num_N, n_obs+S))
    n = 100
    H = 50
    P = 4
    K = 1
    y = [np.zeros((num_N, n_obs+i)) for i in range(S+1)]
    hat = [np.zeros((P, 1)) for _ in range(S)]
    g = np.zeros((S, 1))
    # ar_coeff = [0.2, 0.2, 0.2, 0.2]
    n1 = 0
    l = 1  # Initialize `l` outside the loop
    while n1 < 100:
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=(n_obs+S))  # Normally distributed noise
        ar_series = np.zeros((num_N, n_obs+S))
        for t in range(0, n_obs+S):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]
            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]
            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]
            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]
        for a in range(0, S+1):
            y[a] = ar_series[4][a:n_obs+a]
        X = np.concatenate([ar_series[i][0:n_obs].reshape(-1, 1) for i in range(4)], axis=1)
        V = []
        for a in range(0, S + 1):
            M, _, _ = sir_1_time_series2(X, y[a], H, K)
            V.append(M)
        for q in range(1, S + 1):
            Q = np.zeros((P, P))
            phi = ar_coeff
            for j in range(P):
                for k in range(P):
                    Q[j, k] = sum(sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, l)) for l in range(1, q + 1))
            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K
            K_largest_eigenvectors = eigenvectors1[:, K_index]
            edr_est = K_largest_eigenvectors
            if edr_est[0] < 0:
                edr_est = -edr_est
            edr_est = edr_est / np.linalg.norm(edr_est)
            hat[q - 1] += edr_est
            n1 += 1

    for i in range(S):
        hat[i] = hat[i] / n
        g[i] = hat[i][0] / hat[i][1]
    array = np.array(hat)
    print(tabulate(array, tablefmt='latex'))
    print(g)

# Example usage
ar_coeff = [0.2, 0.2, 0.2, 0.2]
uu2(ar_coeff)

ar_coeff = [0.2, 0.2, 0.8, 0.8]
uu2(ar_coeff)

ar_coeff = [0.2, 0.5, 0.8, 0.8]
uu2(ar_coeff)

ar_coeff = [0.2, 0.5, 0.5, 0.8]
uu2(ar_coeff)









"""### ar_coeff = [0.2,0.2,0.2,0.2]"""

ar_coeff = [0.2,0.2,0.2,0.2]
NEWSIR(ar_coeff, sir_1_time_series1, 10000, 20)

"""### ar_coeff = [0.2,0.2,0.8,0.8]"""

ar_coeff = [0.2,0.2,0.8,0.8]
NEWSIR(ar_coeff, sir_1_time_series1, 10000, 20)

"""### ar_coeff = [0.2,0.5,0.8,0.8]"""

ar_coeff = [0.2,0.5,0.8,0.8]
NEWSIR(ar_coeff, sir_1_time_series1, 10000, 20)

"""### ar_coeff = [0.2,0.5,0.5,0.8]"""

ar_coeff = [0.2,0.5,0.5,0.8]
NEWSIR(ar_coeff, sir_1_time_series1, 10000, 20)

"""# SIR for Time series new objective(Single sum)
## The results are shown by 1. directions 2. ratio between first two coefficients

If the objective is $$\text{diag}(W^T V^{[1: Q]} W)$$

## Use equal slice
"""

#from 0 to Q single sum
from tabulate import tabulate
def NEWSIRsingle(ar_coeff):
    num_N = 5
    n_obs = 1000
    noise = np.zeros((num_N, n_obs))
    n = 100
    H = 50
    P = 4
    K = 1
    S = 20
    hat = [np.zeros((P, 1)) for i in range(S)]
    # ar_coeff = [0.2, 0.2, 0.2, 0.2]
    g = np.zeros((S, 1))
    for w in range(n):
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
        ar_series = np.zeros((num_N, n_obs))
        for t in range(0, n_obs):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1]  + noise[2][t]
            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1]  + noise[3][t]
            ar_series[4][t] = 2*ar_series[0][t] + 3*ar_series[1][t] + noise[4][t]
        y = ar_series[4]
        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
        V = []
        for a in range(0,S+1):
            M, edr, lam1 = sir_1_time_series(X, y, a, H, K)
            V.append(M)
        for q in range(1, S+1):
            Q = np.zeros((P, P))
            phi = ar_coeff
            for j in range(P):
                for k in range(P):
                    Q[j,k] = sum(phi[j]**a * V[a][j,k] * phi[k]**a for a in range(0,q)) #single sum
            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
            K_index = np.argpartition(np.abs(eigenvalues1), P-K) >= P-K
            K_largest_eigenvectors = eigenvectors1[:, K_index]
            edr_est =  K_largest_eigenvectors
            if edr_est[0]<0:
                edr_est = -edr_est
            edr_est = edr_est/np.linalg.norm(edr_est)
            hat[q-1] += edr_est
    for i in range(S):
        hat[i] = hat[i]/n
        g[i] = hat[i][0] / hat[i][1]
    array = np.array(hat)
    print(tabulate(array, tablefmt='latex'))
    print(g)

"""### ar_coeff = [0.2,0.2,0.2,0.2]"""

ar_coeff = [0.2,0.2,0.2,0.2]
NEWSIRsingle(ar_coeff)

"""### ar_coeff = [0.2,0.2,0.8,0.8]"""

ar_coeff = [0.2,0.2,0.8,0.8]
NEWSIRsingle(ar_coeff)

"""### ar_coeff = [0.2,0.5,0.5,0.8]"""

ar_coeff = [0.2,0.5,0.5,0.8]
NEWSIRsingle(ar_coeff)

"""### ar_coeff = [0.2,0.5,0.8,0.8]"""

ar_coeff = [0.2,0.5,0.8,0.8]
NEWSIRsingle(ar_coeff)

"""## Use equal number in each slice"""

#from 0 to Q single sum
from tabulate import tabulate
import numpy as np

def uuu2(ar_coeff):
    import numpy as np
    from tabulate import tabulate

    def sir_1_time_series1(X, y, s, num_slices, K):
        n_samples, n_features = X.shape
        V_hat = np.zeros([X.shape[1], X.shape[1]])
        # Step 1: Sort the data by the response variable
        sorted_indices = np.argsort(y)
        X_sorted = X[sorted_indices]
        y_sorted = y[sorted_indices]
        # Step 2: Divide the data into slices
        slice_size = n_samples // num_slices
        ph_hat = slice_size / n_samples
        slices = []

        for i in range(num_slices):
            start_idx = i * slice_size
            if i < num_slices - 1:
                end_idx = (i + 1) * slice_size
            else:  # Last slice includes any remaining samples
                end_idx = n_samples
            if start_idx - s > 0:
                slices.append((X[[x - s for x in sorted_indices[start_idx:end_idx]]], y_sorted[start_idx:end_idx]))
            else:
                slices.append((X[sorted_indices[0:end_idx - s]], y_sorted[start_idx:end_idx]))

        # Step 3: Compute the means of the predictors within each slice
        X_means = np.array([np.mean(slice_X, axis=0) for slice_X, _ in slices])

        # Step 4: Center the predictor means
        X_centered = X_means - np.mean(X_means, axis=0)

        V_hat = np.add(V_hat, ph_hat * np.matmul(X_centered.T, X_centered))

        #report wrong, there's  NaN or inf value in V_hat, so add this
        # Check for NaN or inf values in V_hat
        if np.isnan(V_hat).any() or np.isinf(V_hat).any():
            # Handle NaN or inf values appropriately
            # For example, replace NaN values with 0 and inf values with a large number
            V_hat[np.isnan(V_hat)] = 0
            V_hat[np.isinf(V_hat)] = np.nanmax(np.abs(V_hat))  # Replace inf with maximum absolute value in V_hat

    # Compute eigenvalues and eigenvectors
        eigenvalues, eigenvectors = np.linalg.eig(V_hat)
        eigenvalues, eigenvectors = np.linalg.eig(V_hat)
        K_index = np.argpartition(np.abs(eigenvalues), X.shape[1] - K) >= X.shape[1] - K
        K_largest_eigenvectors = eigenvectors[:, K_index]
        edr_est = K_largest_eigenvectors

        return V_hat, edr_est, eigenvalues ** 2

    num_N = 5
    n_obs = 1000
    noise = np.zeros((num_N, n_obs))
    n = 100
    H = 50
    P = 4
    K = 1
    S = 20
    hat = [np.zeros((P, 1)) for _ in range(S)]
    g = np.zeros((S, 1))
    # ar_coeff = [0.2, 0.2, 0.2, 0.2]
    n1 = 0
    while n1 < 100:
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
        ar_series = np.zeros((num_N, n_obs))
        for t in range(0, n_obs):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1] + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1] + noise[1][t]
            ar_series[2][t] = ar_coeff[2] * ar_series[2][t - 1] + noise[2][t]
            ar_series[3][t] = ar_coeff[3] * ar_series[3][t - 1] + noise[3][t]
            ar_series[4][t] = 2 * ar_series[0][t] + 3 * ar_series[1][t] + noise[4][t]
        y = ar_series[4]
        X = np.concatenate([ar_series[i].reshape(-1, 1) for i in range(4)], axis=1)
        V = []
        for a in range(0, S + 1):
            M, _, _ = sir_1_time_series1(X, y, a, H, K)
            V.append(M)
        for q in range(1, S + 1):
            Q = np.zeros((P, P))
            phi = ar_coeff
            for j in range(P):
                for k in range(P):
                    Q[j, k] = sum(phi[j] ** a * V[a][j, k] * phi[k] ** a for a in range(0, q))
            eigenvalues1, eigenvectors1 = np.linalg.eig(Q)
            K_index = np.argpartition(np.abs(eigenvalues1), P - K) >= P - K
            K_largest_eigenvectors = eigenvectors1[:, K_index]
            edr_est = K_largest_eigenvectors
            if edr_est[0] < 0:
                edr_est = -edr_est
            edr_est = edr_est / np.linalg.norm(edr_est)
            hat[q - 1] += edr_est
            n1 += 1

    for i in range(S):
        hat[i] = hat[i] / n
        g[i] = hat[i][0] / hat[i][1]
    array = np.array(hat)
    print(tabulate(array, tablefmt='latex'))
    print(g)

ar_coeff = [0.2, 0.2, 0.2, 0.2]
uuu2(ar_coeff)

ar_coeff = [0.2,0.5,0.8,0.8]
uuu2(ar_coeff)

ar_coeff = [0.2,0.2,0.8,0.8]
uuu2(ar_coeff)

ar_coeff = [0.2,0.5,0.5,0.8]
uuu2(ar_coeff)

ar_coeff = [0.2,0.2,0.2,0.2]
uuu2(ar_coeff)

"""# TSIR
## The results are shown by 1. directions
"""

#0.2 0.2 can find edr direction
def TSIR(ar_coeff, T):
    n = 100
    S = 12
    H = 50
    K = 1
    P = 4
    num_N = 5
    n_x = 2
    n_obs = 1000
    QW = np.zeros((P,4))
    for w in range(n):
        noise = np.zeros((num_N, n_obs))
        ar_series = np.zeros((num_N, n_obs))
        Newar_series = np.zeros((n_obs, num_N - 1))
        M = np.zeros((num_N, n_obs))
        X = []
        num_dataframes = S
        y = []
        ar_seriesT1 = pd.DataFrame({})
        ar_seriesT = pd.DataFrame({})
        X1 = np.zeros((P,P))
        for l in range(num_dataframes):
            dg = pd.DataFrame({})
            X.append(dg)
        scaler = StandardScaler()
        lam = pd.DataFrame({})
        # ar_coeff1 = 0.3  # Autoregressive coefficient
        # ar_coeff2 = 0.7
        B = []
        edr1 = []
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
        # Generate the additive AR time series
        for t in range(0, n_obs):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
            ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]
            ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]
            ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]
        y = ar_series[4]
        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
        X = scaler.fit_transform(X)
        # for t in range(0, n_obs):
        #     ar_seriesT1 = pd.concat([ar_seriesT1, pd.DataFrame([[ar_series[0][t], ar_series[1][t], ar_series[2][t], ar_series[3][t]]])], axis = 0, ignore_index=True)
        #     X1 = X1 + ar_seriesT1.values[t].reshape(-1, 1) @ ar_seriesT1.values[t].reshape(-1, 1).T
        # X1 = 1/n_obs*X1
        # eigenvalues, eigenvectors = np.linalg.eig(X1)
        # # Compute reciprocal square root of eigenvalues
        # recip_sqrt_eigenvalues = np.diag(1 / np.sqrt(eigenvalues))
        # # Reconstruct the matrix
        # A_neg_half = eigenvectors @ recip_sqrt_eigenvalues @ np.linalg.inv(eigenvectors)
        # for t in range(0, n_obs):
        #     # M = np.array([Newar_series[i][t] for i in range(num_N - 1)])
        #     Newar_series[t] = A_neg_half @ ar_seriesT1.values[t]
        for j in range(1,S + 1):
            # for T in range(1,n_obs + 1):
            #     if T + j >= (n_obs):
            #         break
            #     else:
                    # y[j-1] = pd.concat([y[j-1], pd.Series([ar_series[4][T+j]])], ignore_index=True)
                    # M = pd.DataFrame({})
                    # for i in range(0,5):
                    #     M[i] = ar_series[i][T]
            # ar_seriesT = pd.DataFrame([[Newar_series[T][0], Newar_series[T][1], Newar_series[T][2], Newar_series[T][3]]])
            V, edr, lam1 = T(X, y, j, H, K=K)
            B.append(V)
            edr1.append(edr)
        from scipy.stats import ortho_group
        def create_random_orthogonal_matrix(rows, cols):
            # Create a random matrix
            random_matrix = np.random.rand(rows, cols)
            # Perform QR decomposition
            q, r = np.linalg.qr(random_matrix)
            return q
        rows = 4
        cols = 4
        P1 = create_random_orthogonal_matrix(rows, cols)
        #W_i
        Gam = np.zeros((4,4))
        r = np.zeros((4,4))
        n1 = 0
        while n1<=10000:
            for i in range(K):
                for k in range(S):
                    Gam[i] += (P1[i] @ B[k] @ P1[i]) * B[k] @ P1[i]
            U, S1, VT = np.linalg.svd(Gam)
            QQ = U @ VT
            P1 = QQ
            n1 = n1 + 1
        for i in range(len(QQ)):
            if QQ[i][0]<0:
                QQ[i] = -QQ[i]
        QQ = QQ/np.linalg.norm(QQ)
        # sum(lam.apply(find_largest, axis=1).values[i]*edr1[i] for i in range(len(edr1)))
        QW += QQ

        # LAM = pd.DataFrame({})
        # for j in range(1, S + 1):
        #     for i in range(4):
        #         LAM1= (QQ[i] @ B[j-1] @ QQ[i].T)**2
        #         LAM = pd.concat([LAM, pd.DataFrame([LAM1])], ignore_index=True)
        # total_sum = LAM.sum().sum()
        # LAM = LAM/total_sum
        # reshaped_data = LAM.values.reshape(S, 4)
        # # Convert reshaped data back to DataFrame
        # LAM = pd.DataFrame(reshaped_data)
        # sorted_columns = LAM.sum().sort_values().index
        # Reorder DataFrame columns based on sorted column names
        # LAM = LAM[sorted_columns]

    QW = QW/n

    return QW

TSIR([0.2,0.2], sir_1_time_series1)

TSIR([0.2,0.8], sir_1_time_series1)

TSIR([0.2,0.2], sir_1_time_series)

TSIR([0.2,0.8], sir_1_time_series)

"""# My idea"""

#0.2 0.2 can find edr direction
def find_largest(row):
        return max(row)
def IDEA(ar_coeff, T):
    n = 100
    S = 12
    H = 50
    K = 1
    P = 4
    num_N = 5
    n_x = 2
    n_obs = 10000
    QW = np.zeros((P,4))
    W1 = np.zeros((P, 1))
    W2 = np.zeros((P, 1))
    for w in range(n):
        noise = np.zeros((num_N, n_obs))
        ar_series = np.zeros((num_N, n_obs))
        Newar_series = np.zeros((n_obs, num_N - 1))
        M = np.zeros((num_N, n_obs))
        X = []
        num_dataframes = S
        y = []
        ar_seriesT1 = pd.DataFrame({})
        ar_seriesT = pd.DataFrame({})
        X1 = np.zeros((P,P))
        for l in range(num_dataframes):
            dg = pd.DataFrame({})
            X.append(dg)
        scaler = StandardScaler()
        lam = pd.DataFrame({})
        # ar_coeff1 = 0.3  # Autoregressive coefficient
        # ar_coeff2 = 0.7
        B = []
        edr1 = []
        for h in range(num_N):
            noise[h] = np.random.normal(0, 1, size=n_obs)  # Normally distributed noise
        # Generate the additive AR time series
        for t in range(0, n_obs):
            ar_series[0][t] = ar_coeff[0] * ar_series[0][t - 1]  + noise[0][t]
            ar_series[1][t] = ar_coeff[1] * ar_series[1][t - 1]  + noise[1][t]
            ar_series[2][t] = 0.3 * ar_series[2][t - 1] + 0.4 * noise[2][t-1] + noise[2][t]
            ar_series[3][t] = -0.4 * noise[3][t-1] + noise[3][t]
            ar_series[4][t] = 2*ar_series[0][t - 1] + 3*ar_series[1][t - 1] + noise[4][t]
        y = ar_series[4]
        X = np.concatenate([ar_series[i].reshape(-1,1) for i in range(4)], axis = 1)
        X = scaler.fit_transform(X)
        # for t in range(0, n_obs):
        #     ar_seriesT1 = pd.concat([ar_seriesT1, pd.DataFrame([[ar_series[0][t], ar_series[1][t], ar_series[2][t], ar_series[3][t]]])], axis = 0, ignore_index=True)
        #     X1 = X1 + ar_seriesT1.values[t].reshape(-1, 1) @ ar_seriesT1.values[t].reshape(-1, 1).T
        # X1 = 1/n_obs*X1
        # eigenvalues, eigenvectors = np.linalg.eig(X1)
        # # Compute reciprocal square root of eigenvalues
        # recip_sqrt_eigenvalues = np.diag(1 / np.sqrt(eigenvalues))
        # # Reconstruct the matrix
        # A_neg_half = eigenvectors @ recip_sqrt_eigenvalues @ np.linalg.inv(eigenvectors)
        # for t in range(0, n_obs):
        #     # M = np.array([Newar_series[i][t] for i in range(num_N - 1)])
        #     Newar_series[t] = A_neg_half @ ar_seriesT1.values[t]
        for j in range(0,S + 1):
            # for T in range(1,n_obs + 1):
            #     if T + j >= (n_obs):
            #         break
            #     else:
                    # y[j-1] = pd.concat([y[j-1], pd.Series([ar_series[4][T+j]])], ignore_index=True)
                    # M = pd.DataFrame({})
                    # for i in range(0,5):
                    #     M[i] = ar_series[i][T]
            # ar_seriesT = pd.DataFrame([[Newar_series[T][0], Newar_series[T][1], Newar_series[T][2], Newar_series[T][3]]])
            V, edr, lam1 = T(X, y, j, H, K=K)
            B.append(V)
            edr1.append(edr)
            lam = pd.concat([lam, pd.DataFrame([lam1])], ignore_index=True)
            total_sum = lam.sum().sum()
            lam = lam/total_sum
            sorted_columns = lam.sum().sort_values().index
            lam = lam[sorted_columns]

    #add weights for each lag?
        W1 += sum(lam.apply(find_largest, axis=1).values[i]*edr1[i] for i in range(len(edr1)))
        W2 += sum(edr1[i] for i in range(len(edr1)))

    W1 = W1/n
    W2 = W2/n

    return W1, W2, lam

#add s=0?
ar_coeff = [0.2, 0.2]
IDEA(ar_coeff, sir_1_time_series1)

ar_coeff = [0.2, 0.8]
IDEA(ar_coeff, sir_1_time_series1)

ar_coeff = [0.8, 0.2]
IDEA(ar_coeff, sir_1_time_series1)
